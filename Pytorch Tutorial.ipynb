{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4158, 0.6673],\n",
      "        [0.8801, 0.0314]])\n",
      "tensor([[0.5434, 0.6437],\n",
      "        [0.4352, 0.8459]])\n",
      "tensor([[0.9592, 1.3110],\n",
      "        [1.3153, 0.8773]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(2, 2)\n",
    "print(x)\n",
    "y = torch.rand(2, 2)\n",
    "print(y)\n",
    "y.add_(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The item() is only available when tensor has one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9591872096061707\n"
     ]
    }
   ],
   "source": [
    "print(y[0,0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7501, 0.6308, 0.1928, 0.6969],\n",
      "        [0.6851, 0.3781, 0.6394, 0.6597],\n",
      "        [0.4858, 0.6606, 0.5606, 0.2121],\n",
      "        [0.4153, 0.7170, 0.6659, 0.8289]]) torch.Size([4, 4])\n",
      "tensor([[0.7501, 0.6308, 0.1928, 0.6969, 0.6851, 0.3781, 0.6394, 0.6597, 0.4858,\n",
      "         0.6606, 0.5606, 0.2121, 0.4153, 0.7170, 0.6659, 0.8289]]) torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "a,b=4,4\n",
    "x = torch.rand(a,b)\n",
    "print(x,x.size())\n",
    "y=x.view(1,16)\n",
    "print(y,y.size())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When one of the values is -1, pytorch will automatically covert -1 to the value that make the product fit to the amount of the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7501, 0.6308],\n",
      "        [0.1928, 0.6969],\n",
      "        [0.6851, 0.3781],\n",
      "        [0.6394, 0.6597],\n",
      "        [0.4858, 0.6606],\n",
      "        [0.5606, 0.2121],\n",
      "        [0.4153, 0.7170],\n",
      "        [0.6659, 0.8289]]) torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "print(x.view(-1,2),x.view(-1,2).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting between tensor and numpy.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "form tensor to numpy.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember if tenseor is save in the CPU instead of the GPU, then a & b will all be saved in the same memory location, that means if one is changed, the other one will change, too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "form numpy.array to tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "print(a)\n",
    "b = torch.from_numpy(a)\n",
    "print(b)\n",
    "\n",
    "a+=1\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can use following code to check if your cuda is available, and switching from CPU to GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But notice! when you're trying to turn z to a numpy.array, you will get Error because numpy is only available on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-93cccc93d0ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    x=torch.ones(5, device = device)\n",
    "    y=torch.ones(5)\n",
    "    y=y.to(device)\n",
    "    z=x+y\n",
    "    z.numpy()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to move z back to CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    x=torch.ones(5, device = device)\n",
    "    y=torch.ones(5)\n",
    "    y=y.to(device)\n",
    "    z=x+y\n",
    "    z = z.to(\"cpu\")\n",
    "    z.numpy()\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you has a variable in your model that you want to optimize, then you need the gredient so you need to specify requires_grad=True(default is False). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5,requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3915, 0.8660, 0.3459], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3,requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch will generate a backpropagation which is called \"AddBackward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.3915, 2.8660, 2.3459], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y=x+2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./images/sample1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we use multiplication, Pytorch will generate a backpropagation which is called \"MulBackward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11.4385, 16.4283, 11.0061], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z=y*y*2 \n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we use mean, Pytorch will generate a backpropagation which is called \"MeanBackward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.9576, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z=z.mean() \n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run following code, we can get the gradients in this tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.1887, 3.8214, 3.1278])\n"
     ]
    }
   ],
   "source": [
    "z.backward() #dz/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one thing you should know that in the background it will create a vector_jacobian to caculate the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./images/vector_jacobian.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we don't run the mean(), we will get an 1 by 3 matrix, which can't call the backward function, because backward function can only be called back for scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11.9533,  8.8635,  9.1756], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3,requires_grad=True)\n",
    "y=x+2\n",
    "z=y*y*2 \n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-40c0c9b0bbab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to give it the gredient argument, so we create an vector of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3, requires_grad=True)\n",
    "y=x+2\n",
    "z=y*y*2 \n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
    "z.backward(v) #dz/zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0661e+00, 1.0928e+01, 1.0672e-02])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prevent tensor from tracking the grandients(or the tensor will keep creating backward function when you're running a new operation), here are three ways you can try on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8698, 0.3098, 0.9790], requires_grad=True)\n",
      "tensor([0.8698, 0.3098, 0.9790])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3,requires_grad=True)\n",
    "print(x)\n",
    "x.requires_grad_(False)\n",
    "\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2703, 0.8299, 0.8660], requires_grad=True)\n",
      "tensor([0.2703, 0.8299, 0.8660])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, requires_grad=True)\n",
    "print(x)\n",
    "y=x.detach()\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6355, 0.4803, 0.5042], requires_grad=True)\n",
      "tensor([2.6355, 2.4803, 2.5042])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, requires_grad=True)\n",
    "print(x)\n",
    "with torch.no_grad():\n",
    "    y = x+2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we call the backward function, the gradient for this tensor will be accumulated into the .grad() attribute which will let their values be summed up and will let the gradients of the tensor be incorrect, so before we start the next iteration or optimization step, we must empty the gradients ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "    weights.grad.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent with Autograd & Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x=torch.tensor(1.0)\n",
    "y=torch.tensor(2.0)\n",
    "w=torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# forward pass and compute the loss\n",
    "y_hat=w*x\n",
    "loss=(y_hat-y)**2\n",
    "print(loss) \n",
    "\n",
    "# backward pass\n",
    "loss.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before traning: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 2: w = 1.680, loss = 4.79999924\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 4: w = 1.949, loss = 0.12288000\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 6: w = 1.992, loss = 0.00314570\n",
      "epoch 7: w = 1.997, loss = 0.00050332\n",
      "epoch 8: w = 1.999, loss = 0.00008053\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 10: w = 2.000, loss = 0.00000206\n",
      "Prediction after traning: f(5) = 9.999\n"
     ]
    }
   ],
   "source": [
    "# f = 2 * x \n",
    "X= np.array([1,2,3,4],dtype=np.float32)\n",
    "Y= np.array([2,4,6,8],dtype=np.float32)\n",
    "\n",
    "w=0.0\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "# loss=MSE\n",
    "def loss(y,y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "# gradient\n",
    "# MSE = 1/N * (w*x-y)**2\n",
    "# dJ/dw = 1/N 2x(w*x-y)\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "print(f'Prediction before traning: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "#     prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "#     loss\n",
    "    l = loss(Y,y_pred)\n",
    "    \n",
    "#     gradients \n",
    "    dw = gradient(X,Y,y_pred)\n",
    "    \n",
    "#     update weights\n",
    "    w -= learning_rate * dw\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}\")\n",
    "        \n",
    "print(f'Prediction after traning: f(5) = {forward(5):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you will find out that the Prediction after traning still doesn't equal to the true value, so we modify the n_iters from 10 to 20, and see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before traning: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 2: w = 1.680, loss = 4.79999924\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 4: w = 1.949, loss = 0.12288000\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 6: w = 1.992, loss = 0.00314570\n",
      "epoch 7: w = 1.997, loss = 0.00050332\n",
      "epoch 8: w = 1.999, loss = 0.00008053\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 10: w = 2.000, loss = 0.00000206\n",
      "epoch 11: w = 2.000, loss = 0.00000033\n",
      "epoch 12: w = 2.000, loss = 0.00000005\n",
      "epoch 13: w = 2.000, loss = 0.00000001\n",
      "epoch 14: w = 2.000, loss = 0.00000000\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 16: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 18: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "epoch 20: w = 2.000, loss = 0.00000000\n",
      "Prediction after traning: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# f = 2 * x \n",
    "X= np.array([1,2,3,4],dtype=np.float32)\n",
    "Y= np.array([2,4,6,8],dtype=np.float32)\n",
    "\n",
    "w=0.0\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "# loss=MSE\n",
    "def loss(y,y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "# gradient\n",
    "# MSE = 1/N * (w*x-y)**2\n",
    "# dJ/dw = 1/N 2x(w*x-y)\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "print(f'Prediction before traning: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "#     prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "#     loss\n",
    "    l = loss(Y,y_pred)\n",
    "    \n",
    "#     gradients \n",
    "    dw = gradient(X,Y,y_pred)\n",
    "    \n",
    "#     update weights\n",
    "    w -= learning_rate * dw\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}\")\n",
    "        \n",
    "print(f'Prediction after traning: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then the prediction equals to the true value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do everything with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before traning: f(5) = 0.000\n",
      "epoch 1: w = 0.450, loss = 67.50000000\n",
      "epoch 11: w = 2.498, loss = 2.61626792\n",
      "epoch 21: w = 2.901, loss = 0.10140543\n",
      "epoch 31: w = 2.981, loss = 0.00393051\n",
      "epoch 41: w = 2.996, loss = 0.00015233\n",
      "epoch 51: w = 2.999, loss = 0.00000590\n",
      "epoch 61: w = 3.000, loss = 0.00000023\n",
      "epoch 71: w = 3.000, loss = 0.00000001\n",
      "epoch 81: w = 3.000, loss = 0.00000000\n",
      "epoch 91: w = 3.000, loss = 0.00000000\n",
      "Prediction after traning: f(5) = 15.000\n"
     ]
    }
   ],
   "source": [
    "# f = 2 * x \n",
    "X= torch.tensor([1,2,3,4],dtype=torch.float32)\n",
    "Y= torch.tensor([3,6,9,12],dtype=torch.float32)\n",
    "\n",
    "w= torch.tensor(0.0,dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "# loss=MSE\n",
    "def loss(y,y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "# gradient\n",
    "# MSE = 1/N * (w*x-y)**2\n",
    "# dJ/dw = 1/N 2x(w*x-y)\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "print(f'Prediction before traning: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "#     prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "#     loss\n",
    "    l = loss(Y,y_pred)\n",
    "    \n",
    "#     gradients = backward pass\n",
    "    l.backward() #dl/dw\n",
    "    \n",
    "#     update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "\n",
    "#     zero gradients\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}\")\n",
    "\n",
    "print(f'Prediction after traning: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline: model, loss, and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Design model (input,output size, forward pass)\n",
    "2. Construct loss and optimizer\n",
    "3. Training loop\n",
    "    - forward pass : compute function\n",
    "    - backward pass : gradients\n",
    "    - update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before traning: f(5) = -3.614\n",
      "epoch 1: w = 0.553, loss = 108.99124146\n",
      "epoch 11: w = 1.746, loss = 5.09082365\n",
      "epoch 21: w = 1.403, loss = 3.54097795\n",
      "epoch 31: w = 2.077, loss = 1.30052698\n",
      "epoch 41: w = 2.664, loss = 0.23959801\n",
      "epoch 51: w = 2.932, loss = 0.02147231\n",
      "epoch 61: w = 3.006, loss = 0.00097522\n",
      "epoch 71: w = 3.013, loss = 0.00021787\n",
      "epoch 81: w = 3.006, loss = 0.00006494\n",
      "epoch 91: w = 3.002, loss = 0.00000830\n",
      "Prediction after traning: f(5) = 15.001\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# f = 2 * x \n",
    "X= torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\n",
    "Y= torch.tensor([[3],[6],[9],[12]],dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5],dtype=torch.float32)\n",
    "\n",
    "n_samples,n_features = X.shape\n",
    "print(n_samples,n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearRegression(nn.Module): #basically, all \"def\"s in pytorch are realized by inheriting from \"nn.Module\". \n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "#       define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):   # Very important!!! forward(self, *input) in pytorch == def __call__(self),\n",
    "        return self.lin(x)  # and x == the matrix of weights generate by nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    \n",
    "model = LinearRegression(input_size, output_size)\n",
    "    \n",
    "print(f'Prediction before traning: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.02\n",
    "n_iters = 100\n",
    "\n",
    "#     loss\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "#     update weights\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# model.parameters() is used to get the weights after model do LinearRegression.\n",
    "\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "#     prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "    \n",
    "#     loss\n",
    "    l = loss(Y,y_pred)\n",
    "    \n",
    "#     gradients = backward pass\n",
    "    l.backward() #dl/dw\n",
    "    \n",
    "#     update weights\n",
    "    optimizer.step()\n",
    "\n",
    "#     zero gradients\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        [w, b]=model.parameters()\n",
    "        print(f\"epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}\")\n",
    "\n",
    "print(f'Prediction after traning: f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0004],\n",
       "        [ 6.0005],\n",
       "        [ 9.0006],\n",
       "        [12.0007]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0001041889190674"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[0][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 40])\n"
     ]
    }
   ],
   "source": [
    "# 備註nn.linear()操作\n",
    "m = nn.Linear(20, 40) #建構一個線性方程式\"y=xA^T+b\", 而(20,40)指的是A的shape, 也就是weights的矩陣\n",
    "input = torch.randn(128, 20) #隨機生成一個128*20的tensor\n",
    "output = m(input) #做矩陣乘法\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Design model (input,output size, forward pass)\n",
    "2. Construct loss and optimizer\n",
    "3. Training loop\n",
    "    - forward pass : compute function\n",
    "    - backward pass : gradients\n",
    "    - update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss = 5815.5386\n",
      "epoch: 11, loss = 4307.4888\n",
      "epoch: 21, loss = 3216.8760\n",
      "epoch: 31, loss = 2427.2820\n",
      "epoch: 41, loss = 1855.0376\n",
      "epoch: 51, loss = 1439.9211\n",
      "epoch: 61, loss = 1138.5247\n",
      "epoch: 71, loss = 919.5188\n",
      "epoch: 81, loss = 760.2628\n",
      "epoch: 91, loss = 644.3764\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "# 0) prepare data\n",
    "\n",
    "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
    "\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y = y.view(y.shape[0],1)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# 1) model\n",
    "\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# 2) loss and optimizer \n",
    "\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) training loop\n",
    "n_epoch = 100\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "#     prediction = forward pass and loss\n",
    "    y_predicted = model(X)\n",
    "    loss = criterion(y_predicted, y)\n",
    "    \n",
    "#     backward pass\n",
    "    loss.backward() #dl/dw\n",
    "    \n",
    "#     update weights\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        [w, b]=model.parameters()\n",
    "        print(f\"epoch: {epoch+1}, loss = {loss.item():.4f}\")\n",
    "        \n",
    "# 4) plot\n",
    "predicted = model(X).detach().numpy() #Remove the gradients from the results of the prediction, and turn the results to numpy.array \n",
    "plt.plot(X_numpy, y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic  Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is normallly used to resolve the classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 30\n",
      "epoch: 1, loss = 0.6998\n",
      "epoch: 11, loss = 0.5509\n",
      "epoch: 21, loss = 0.4631\n",
      "epoch: 31, loss = 0.4057\n",
      "epoch: 41, loss = 0.3649\n",
      "epoch: 51, loss = 0.3343\n",
      "epoch: 61, loss = 0.3102\n",
      "epoch: 71, loss = 0.2907\n",
      "epoch: 81, loss = 0.2744\n",
      "epoch: 91, loss = 0.2606\n",
      "epoch: 101, loss = 0.2488\n",
      "epoch: 111, loss = 0.2384\n",
      "epoch: 121, loss = 0.2292\n",
      "epoch: 131, loss = 0.2210\n",
      "epoch: 141, loss = 0.2137\n",
      "epoch: 151, loss = 0.2070\n",
      "epoch: 161, loss = 0.2009\n",
      "epoch: 171, loss = 0.1954\n",
      "epoch: 181, loss = 0.1903\n",
      "epoch: 191, loss = 0.1855\n",
      "epoch: 201, loss = 0.1812\n",
      "epoch: 211, loss = 0.1771\n",
      "epoch: 221, loss = 0.1733\n",
      "epoch: 231, loss = 0.1697\n",
      "epoch: 241, loss = 0.1663\n",
      "epoch: 251, loss = 0.1632\n",
      "epoch: 261, loss = 0.1602\n",
      "epoch: 271, loss = 0.1574\n",
      "epoch: 281, loss = 0.1547\n",
      "epoch: 291, loss = 0.1522\n",
      "accuracy = 0.9211\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 0) prepare data\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "print(n_samples, n_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1234)\n",
    "\n",
    "# Scale\n",
    "\n",
    "sc = StandardScaler() # this will make your data's mean equals zero.\n",
    "X_train = sc.fit_transform(X_train)# fit means '擬合'\n",
    "X_test = sc.transform(X_test)      # the reason why here do not need fit is because this is test data, \n",
    "                                   # so we don't want the model to coordinate with test data, or the model will be useless. \n",
    "    \n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0],1)\n",
    "y_test = y_test.view(y_test.shape[0],1)\n",
    "\n",
    "\n",
    "# 1) model\n",
    "#    f = wx + b, sigmoid at the end \n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__() #繼承了LogisticRegression的父級,也就是nn.Module的__init__()\n",
    "#       define layers\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "\n",
    "    def forward(self, x):   \n",
    "        y_predicted = torch.sigmoid(self.linear(x)) #the reason why we need sigmoid is because y(logit(Oods)) may smaller than 0. \n",
    "        return y_predicted\n",
    "    \n",
    "model =  LogisticRegression(n_features)       \n",
    "\n",
    "# 2) loss and optimizer \n",
    "\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()  # binary cross-entropy loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) training loop\n",
    "n_epoch = 300\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "#     prediction = forward pass and loss\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    \n",
    "#     backward pass\n",
    "    loss.backward() #dl/dw\n",
    "    \n",
    "#     update weights\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        [w, b]=model.parameters()\n",
    "        print(f\"epoch: {epoch+1}, loss = {loss.item():.4f}\")\n",
    "        \n",
    "with torch.no_grad(): # without this code y_predicted.round() will still compute the gradient and track it.\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0]) #compare each item is equaled or not,\n",
    "    \n",
    "print(f'accuracy = {acc:.4f}')\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./images/logistic_regression.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./images/sigmoid.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader - Batch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
      "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
      "        1.0650e+03]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "class WineDataset(Dataset):\n",
    "    def __init__(self):\n",
    "#         data loading\n",
    "        xy=np.loadtxt('./data/wine.csv', delimiter=\",\",dtype=np.float32,skiprows=1)\n",
    "        self.x = torch.from_numpy(xy[: , 1:])\n",
    "        self.y = torch.from_numpy(xy[: , [0]])\n",
    "        self.n_samples = xy.shape[0]\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "# dataset[0]\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "# len(dataset)\n",
    "        return self.n_samples \n",
    "\n",
    "\n",
    "dataset = WineDataset()\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features, labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2250e+01, 1.7300e+00, 2.1200e+00, 1.9000e+01, 8.0000e+01, 1.6500e+00,\n",
      "         2.0300e+00, 3.7000e-01, 1.6300e+00, 3.4000e+00, 1.0000e+00, 3.1700e+00,\n",
      "         5.1000e+02],\n",
      "        [1.1870e+01, 4.3100e+00, 2.3900e+00, 2.1000e+01, 8.2000e+01, 2.8600e+00,\n",
      "         3.0300e+00, 2.1000e-01, 2.9100e+00, 2.8000e+00, 7.5000e-01, 3.6400e+00,\n",
      "         3.8000e+02],\n",
      "        [1.3230e+01, 3.3000e+00, 2.2800e+00, 1.8500e+01, 9.8000e+01, 1.8000e+00,\n",
      "         8.3000e-01, 6.1000e-01, 1.8700e+00, 1.0520e+01, 5.6000e-01, 1.5100e+00,\n",
      "         6.7500e+02],\n",
      "        [1.3240e+01, 2.5900e+00, 2.8700e+00, 2.1000e+01, 1.1800e+02, 2.8000e+00,\n",
      "         2.6900e+00, 3.9000e-01, 1.8200e+00, 4.3200e+00, 1.0400e+00, 2.9300e+00,\n",
      "         7.3500e+02]]) tensor([[2.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [1.]])\n",
      "178 45\n",
      "epoch 1/4, step 5/45, inputs torch.Size([4, 13]), labels: tensor([[1.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [2.]])\n",
      "epoch 1/4, step 10/45, inputs torch.Size([4, 13]), labels: tensor([[3.],\n",
      "        [3.],\n",
      "        [1.],\n",
      "        [3.]])\n",
      "epoch 1/4, step 15/45, inputs torch.Size([4, 13]), labels: tensor([[3.],\n",
      "        [3.],\n",
      "        [2.],\n",
      "        [2.]])\n",
      "epoch 1/4, step 20/45, inputs torch.Size([4, 13]), labels: tensor([[2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.]])\n",
      "epoch 1/4, step 25/45, inputs torch.Size([4, 13]), labels: tensor([[2.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [1.]])\n",
      "epoch 1/4, step 30/45, inputs torch.Size([4, 13]), labels: tensor([[1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.]])\n",
      "epoch 1/4, step 35/45, inputs torch.Size([4, 13]), labels: tensor([[1.],\n",
      "        [1.],\n",
      "        [3.],\n",
      "        [1.]])\n",
      "epoch 1/4, step 40/45, inputs torch.Size([4, 13]), labels: tensor([[2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "epoch 1/4, step 45/45, inputs torch.Size([2, 13]), labels: tensor([[2.],\n",
      "        [3.]])\n",
      "epoch 2/4, step 5/45, inputs torch.Size([4, 13]), labels: tensor([[2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "epoch 2/4, step 10/45, inputs torch.Size([4, 13]), labels: tensor([[3.],\n",
      "        [1.],\n",
      "        [3.],\n",
      "        [1.]])\n",
      "epoch 2/4, step 15/45, inputs torch.Size([4, 13]), labels: tensor([[1.],\n",
      "        [3.],\n",
      "        [1.],\n",
      "        [3.]])\n",
      "epoch 2/4, step 20/45, inputs torch.Size([4, 13]), labels: tensor([[2.],\n",
      "        [3.],\n",
      "        [2.],\n",
      "        [2.]])\n",
      "epoch 2/4, step 25/45, inputs torch.Size([4, 13]), labels: tensor([[3.],\n",
      "        [3.],\n",
      "        [2.],\n",
      "        [2.]])\n",
      "epoch 2/4, step 30/45, inputs torch.Size([4, 13]), labels: tensor([[3.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "epoch 2/4, step 35/45, inputs torch.Size([4, 13]), labels: tensor([[3.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [2.]])\n",
      "epoch 2/4, step 40/45, inputs torch.Size([4, 13]), labels: tensor([[3.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "epoch 2/4, step 45/45, inputs torch.Size([2, 13]), labels: tensor([[1.],\n",
      "        [2.]])\n",
      "epoch 3/4, step 5/45, inputs torch.Size([4, 13]), labels: tensor([[1.],\n",
      "        [1.],\n",
      "        [3.],\n",
      "        [1.]])\n",
      "epoch 3/4, step 10/45, inputs torch.Size([4, 13]), labels: tensor([[1.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [2.]])\n",
      "epoch 3/4, step 15/45, inputs torch.Size([4, 13]), labels: tensor([[3.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [3.]])\n",
      "epoch 3/4, step 20/45, inputs torch.Size([4, 13]), labels: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "epoch 3/4, step 25/45, inputs torch.Size([4, 13]), labels: tensor([[1.],\n",
      "        [1.],\n",
      "        [3.],\n",
      "        [3.]])\n",
      "epoch 3/4, step 30/45, inputs torch.Size([4, 13]), labels: tensor([[1.],\n",
      "        [3.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "epoch 3/4, step 35/45, inputs torch.Size([4, 13]), labels: tensor([[3.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.]])\n",
      "epoch 3/4, step 40/45, inputs torch.Size([4, 13]), labels: tensor([[3.],\n",
      "        [1.],\n",
      "        [3.],\n",
      "        [2.]])\n",
      "epoch 3/4, step 45/45, inputs torch.Size([2, 13]), labels: tensor([[3.],\n",
      "        [2.]])\n",
      "epoch 4/4, step 5/45, inputs torch.Size([4, 13]), labels: tensor([[2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "epoch 4/4, step 10/45, inputs torch.Size([4, 13]), labels: tensor([[3.],\n",
      "        [3.],\n",
      "        [2.],\n",
      "        [2.]])\n",
      "epoch 4/4, step 15/45, inputs torch.Size([4, 13]), labels: tensor([[2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [3.]])\n",
      "epoch 4/4, step 20/45, inputs torch.Size([4, 13]), labels: tensor([[1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.]])\n",
      "epoch 4/4, step 25/45, inputs torch.Size([4, 13]), labels: tensor([[2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [3.]])\n",
      "epoch 4/4, step 30/45, inputs torch.Size([4, 13]), labels: tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [1.]])\n",
      "epoch 4/4, step 35/45, inputs torch.Size([4, 13]), labels: tensor([[2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.]])\n",
      "epoch 4/4, step 40/45, inputs torch.Size([4, 13]), labels: tensor([[2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [3.]])\n",
      "epoch 4/4, step 45/45, inputs torch.Size([2, 13]), labels: tensor([[3.],\n",
      "        [1.]])\n",
      "45\n",
      "178\n"
     ]
    }
   ],
   "source": [
    "class WineDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "#         data loading\n",
    "        xy=np.loadtxt('./data/wine.csv', delimiter=\",\",dtype=np.float32,skiprows=1)\n",
    "        self.x = torch.from_numpy(xy[: , 1:])\n",
    "        self.y = torch.from_numpy(xy[: , [0]])\n",
    "        self.n_samples = xy.shape[0]\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "# dataset[0]\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "# len(dataset)\n",
    "        return self.n_samples \n",
    "\n",
    "\n",
    "    dataset = WineDataset()\n",
    "    dataloader = DataLoader(dataset = dataset, batch_size = 4, shuffle = True, num_workers = 0)\n",
    "    # batch_size 是分成幾批 shuffle = True 是指隨機分配 num_workers 是指有多少workers來處理batches\n",
    "\n",
    "    dataiter = iter(dataloader) # 生成一個迭代物件\n",
    "    data = dataiter.next() # next()必須搭配 iter()\n",
    "    features, labels = data\n",
    "    print(features, labels)\n",
    "\n",
    "\n",
    "\n",
    "# training loop\n",
    "    num_epochs = 4 # 是指要將資料訓練幾個循環\n",
    "    total_samples = len(dataset)\n",
    "    n_iteration = math.ceil(total_samples/4)\n",
    "\n",
    "    print(total_samples, n_iteration)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (inputs, labels) in enumerate(dataloader): # enumerate()可列出一个為索引序列及元素的組合\n",
    "#             forward, backward, update\n",
    "            if (i+1)%5 == 0:\n",
    "                print(f\"epoch {epoch+1}/{num_epochs}, step {i+1}/{n_iteration}, inputs {inputs.shape}, labels: {labels}\")\n",
    "\n",
    "    print(len(dataloader))\n",
    "    print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data from numpy, images to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      " 2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "tensor([2.8460e+01, 3.4200e+00, 4.8600e+00, 3.1200e+01, 2.5400e+02, 5.6000e+00,\n",
      "        6.1200e+00, 5.6000e-01, 4.5800e+00, 1.1280e+01, 2.0800e+00, 7.8400e+00,\n",
      "        2.1300e+03])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "class WineDataset(Dataset):\n",
    "\n",
    "    def __init__(self, transform = None):\n",
    "#         data loading\n",
    "        xy=np.loadtxt('./data/wine.csv', delimiter=\",\",dtype=np.float32,skiprows=1)\n",
    "        self.x = xy[: , 1:]\n",
    "        self.y = xy[: , [0]]\n",
    "        self.n_samples = xy.shape[0]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "# dataset[0]\n",
    "        sample = self.x[index], self.y[index]\n",
    "    \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    " \n",
    "    def __len__(self):\n",
    "# len(dataset)\n",
    "        return self.n_samples \n",
    "\n",
    "class ToTensor: \n",
    "    def __call__(self, sample): \n",
    "        #let ToTensor become a callable Object\n",
    "        inputs, targets = sample\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
    "    \n",
    "class Multransform:\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "          \n",
    "    def __call__(self, sample): \n",
    "        #let ToTensor become a callable Object\n",
    "        inputs, targets = sample\n",
    "        inputs *= self.factor\n",
    "        return inputs, targets\n",
    "    \n",
    "\n",
    "\n",
    "dataset = WineDataset(transform = None)\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features)\n",
    "print(type(features),type(labels))\n",
    "\n",
    "composed = torchvision.transforms.Compose([ToTensor(), Multransform(2)])\n",
    "dataset = WineDataset(transform = composed)\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features)\n",
    "print(type(features),type(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax and Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax activation let the data output from actual value to be probabilities.\n",
    "Notice !!! It can only be used as output layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./images/softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./images/softmax_layer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax numpy: [0.65900114 0.24243297 0.09856589]\n",
      "softmax torch tensor([0.6590, 0.2424, 0.0986])\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis = 0)\n",
    "\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "output = softmax(x)\n",
    "print('softmax numpy:', output)\n",
    "\n",
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "output = torch.softmax(x, dim=0)\n",
    "print('softmax torch', output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./images/cross_entropy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1 numpy: 0.357\n",
      "loss2 numpy: 2.303\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def cross_entropy(actual, predicted):\n",
    "    loss = -np.sum(actual * np.log(predicted))\n",
    "    return loss # / float(predicted.shape[0])\n",
    "\n",
    "# y must be one-hot encoded\n",
    "# if class 0: [1 0 0]\n",
    "# if class 1: [0 1 0]\n",
    "# if class 2: [0 0 1]\n",
    "\n",
    "Y =  np.array([1, 0, 0])\n",
    "\n",
    "# y_pred has probabilities\n",
    "Y_pred_good = np.array([0.7, 0.2, 0.1])\n",
    "Y_pred_bad = np.array([0.1, 0.3, 0.6])\n",
    "l1 = cross_entropy(Y, Y_pred_good)\n",
    "l2 = cross_entropy(Y, Y_pred_bad)\n",
    "print(f\"loss1 numpy: {l1:.3f}\")\n",
    "print(f\"loss2 numpy: {l2:.3f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02431126870214939\n",
      "3.4183647632598877\n",
      "tensor([2, 0, 1])\n",
      "tensor([1, 2, 0])\n",
      "torch.return_types.max(\n",
      "values=tensor([3., 9., 5.]),\n",
      "indices=tensor([1, 2, 0]))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# CrossEntropyLoss in PyTorch (applies Softmax)\n",
    "# nn.LogSoftmax + nn.NLLLoss\n",
    "# NLLLoss = negative log likelihood loss\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "Y = torch.tensor([2, 0, 1]) #  n_samples == 3\n",
    "#here maens that [the third value in 1st class, first value in 2nd class, second value in 3rd class] has the maximum\n",
    "\n",
    "# n_samples*n_classes = 3*3\n",
    "Y_pred_good = torch.tensor([[5.0, 1.0, 9.0], [5.0, 1.0, 0.25], [0.0, 7.0, 3.4]]) \n",
    "Y_pred_bad = torch.tensor([[0.5, 3.0, 1.0], [5.0, 1.0, 9.0], [5.0, 1.0, 1.2]])\n",
    "\n",
    "\n",
    "\n",
    "l1 = loss(Y_pred_good, Y)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "\n",
    "print(l1.item())\n",
    "print(l2.item())\n",
    "\n",
    "_, prediction1 = torch.max(Y_pred_good, 1)\n",
    "_, prediction2 = torch.max(Y_pred_bad, 1)\n",
    "prediction = torch.max(Y_pred_bad, 1)\n",
    "\n",
    "print(prediction1)\n",
    "print(prediction2)\n",
    "print(prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./images/nn.CrossEntropyLoss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check https://blog.csdn.net/silver1225/article/details/88914652 to deeply understand what is NLLLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check https://zhuanlan.zhihu.com/p/35709485 to learn more about CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "# Binary problem\n",
    "# option1\n",
    "class NeuralNet1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet1, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "#         no softmax() at the end\n",
    "#       we must use sigmoid at the end, because we need to restrict our output between 0 & 1\n",
    "        y_pred = torch.sigmoid(out)\n",
    "        return y_pred\n",
    "\n",
    "# option2\n",
    "class NeuralNet1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet1, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.ReLu(self.linear1(x))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        return y_pred\n",
    "        \n",
    "model = NeuralNet1(input_size = 28*28, hidden_size = 5)\n",
    "criterion = nn.BCELoss() #Binary Cross Entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "# multiclass problem\n",
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "#         no softmax() at the end\n",
    "        return out\n",
    "\n",
    "model = NeuralNet2(input_size = 28*28, hidden_size = 5, num_classes = 3)\n",
    "criterion = nn.CrossEntropyLoss() #applies softmax & NLLLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-Forward Neural Network (Using MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD6CAYAAAC4RRw1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAbB0lEQVR4nO3de4xWxfkH8O8jLt74xQIC3QIBAYtuqYoCRUSxVeSiCF6oqDF4SbENWIwU5WJjb6aEJjRtRewmEtAStALqqlQgBKW2YFgqKLgglwhsXF0oVkElsDC/P/Y4zBz2vPvu+57bnPf7STb7zDtn3/Poszt7mJ0zR5RSICIi95yWdAJERFQYDuBERI7iAE5E5CgO4EREjuIATkTkKA7gRESOKmoAF5HhIrJdRHaKyLSwkqJksa7ZxdpmixS6DlxEWgH4EMBQALUANgC4Qyn1QXjpUdxY1+xibbPn9CK+dgCAnUqp3QAgIs8DGA0g8JtBRHjXUEoopSSgi3V1WI66Ai2sLeuaKgeUUh38LxYzhdIZwD6jXeu9ZhGRCSJSLSLVRZyL4sO6ZleztWVdU2tPUy8WcwXe1G/6U35jK6UqAVQC/I3uCNY1u5qtLevqlmKuwGsBdDXaXQB8XFw6lAKsa3axthlTzAC+AcAFInK+iLQGMA5AVThpUYJY1+xibTOm4CkUpVSDiEwCsAJAKwDzlVJbQ8uMEsG6Zhdrmz0FLyMs6GScU0uNZlYrtAjrmh6sa2ZtVEr187/IOzGJiBzFAZyIyFEcwImIHFXMOnCi1PnFL35htc866ywdX3zxxVbfbbfdFvg+8+bNs9rr1q3T8XPPPVdMikSh4RU4EZGjOIATETmKywhLVJaWm73wwgs6zjUtUoxdu3bp+LrrrrP69u7dG8k5C5Glusbhu9/9ro63bdtm9U2ePFnHf/nLX2LLKQCXERIRZQkHcCIiR3EAJyJyFJcRknPMOW8g/3lv/xznihUrdNyjRw+rb9SoUVa7Z8+eOr7rrrusvt///vd5nZ/Sp2/fvjo+ceKE1VdbWxt3Oi3GK3AiIkdxACcichSnUMgJ/fqdXEF18803Bx63dau9O+pNN92k4wMHDlh9hw8f1nHr1q2tvvXr11vtSy65RMft27fPI2NywaWXXqrjL7/80up76aWX4k6nxXgFTkTkKA7gRESO4gBOROQo5+fA/UvIfvKTn+j444/t57UeOXJEx4sWLbL6PvnkEx3v3LkzzBQpBOXl5ToWse8WN+e9hw0bZvXV1dXl9f5Tpkyx2hUVFYHHvv7663m9J6VPnz59rPakSZN07OIuk7wCJyJyFAdwIiJHOT+FMnv2bKvdvXv3vL7ugQcesNqHDh3SsX8pWhzMu778/03V1dVxp5M6r776qo579epl9Zm1O3jwYEHvP27cOKtdVlZW0PtQul144YVW+5xzztGx/w5fF/AKnIjIURzAiYgcxQGciMhRzs+Bm8sGAfvBtTU1NVbfRRddpOPLLrvM6rvmmmt0PHDgQKtv3759Ou7atWveuTU0NFjt/fv369hcFufnf8IL58Bte/bsCeV9pk6dqmPzySxNeeedd5qMyS2PPPKI1Ta/l1z8OeMVOBGRo5odwEVkvojUi8gW47V2IrJKRHZ4n9tGmyaFjXXNLta2dDT7UGMRuRrAYQDPKqX6eK/NBnBQKTVLRKYBaKuUerTZk6X4Ialt2578fjZ3KAOAjRs36rh///55v6d55ycAfPjhhzr2T++0a9dOxxMnTrT65s2bl/c5W2AISqCuphtvvNFqv/jiizr270ZYX19vtc1lhm+99VYE2YVDKSVh/cy6Utdc/MuKd+/ebbXNn0n/EsOUKeyhxkqptQD8i2tHA1joxQsBjCk6PYoV65pdrG3pKHQOvJNSqg4AvM8dw0uJEsS6Zhdrm0GRr0IRkQkAJkR9HooX65pNrKtbCh3APxWRcqVUnYiUA6gPOlApVQmgEkj3nNpnn32m4zVr1gQet3r16oLPceutt+rYnHMHgPfff1/HCd7Sm7m6msyn+gCnznub/DVI87x3nvKqrYt1zWXIkCE5+82lvS4qdAqlCsB4Lx4P4JVw0qGEsa7ZxdpmUD7LCBcDWAegt4jUisj9AGYBGCoiOwAM9drkENY1u1jb0tHsFIpS6o6ArmtDziVzOna0/0701FNP6fi00+zfnb/5zW90XOiOei1RKnV9+eWXdXz99dcHHvfss89a7cceeyyynKJWKrXNx/e///2c/f6dP13DOzGJiBzFAZyIyFEcwImIHOX8boRp5r8lvkOHDjo2ly0CwPbt22PJKev8uzwOGjRIx2eccYbVd+DAAR3/7ne/s/oOHz4cQXYUB3M30Xvvvdfqe/fdd632qlWrYskpKrwCJyJyFAdwIiJHcQolZFdeeaWOp02bFnjcmDH2XkJbtmwJOJJaYunSpVa7ffv2gcf+7W9/0/GuXbsiy4nidd111+nY3OUTAN544w2r7d8x1DW8AicichQHcCIiR3EAJyJyFOfAQzZy5Egdl5WVWX3mTobr1q2LLaesu+mmm3Tsf1i16c0337Tajz/+eFQpUYIuueQSHfufOLZkyZK404kUr8CJiBzFAZyIyFEcwImIHMU58CKdddZZVnv48OE6Pnr0qNVnzrkeO3Ys2sQyzL+2e8aMGTr2/93BtGnTJqvN2+Wz4dvf/rbVvuqqq3Ts36LipZdeiiWnuPAKnIjIURzAiYgcxSmUIk2dOtVq9+3bV8f+23b//e9/x5JT1k2ZMsVq9+/fP/BY84k8XDaYTffcc4/VNp+E9Y9//CPmbOLFK3AiIkdxACcichQHcCIiR3EOvIVuuOEGq/3LX/7San/xxRc6Np80T+F5+OGH8z520qRJOuaywWzq1q1bYJ//yVdZwytwIiJHcQAnInIUp1DyYN759+c//9nqa9WqldVevny5jtevXx9tYtQs84ksxdz9+vnnnwe+j3n357nnnhv4Ht/61resdr5TQcePH7fajz76qI6/+uqrvN4jy2688cbAvldffTXGTOLHK3AiIkdxACciclSzA7iIdBWRNSJSIyJbRWSy93o7EVklIju8z22jT5fCwrpmE+taWvKZA28AMEUp9R8R+T8AG0VkFYB7AKxWSs0SkWkApgF4NMf7OMM/r23eEn/++edbff6nmfuXFaZYSdT1vffeC+V9XnzxRR3X1dVZfZ06ddLx7bffHsr5cvnkk090/MQTT/i7S6KugwcP1rF/N8JS0uwVuFKqTin1Hy8+BKAGQGcAowEs9A5bCGBMVElS+FjXbGJdS0uLVqGISHcAfQG8A6CTUqoOaPymEZGOAV8zAcCE4tKkKLGu2cS6Zl/eA7iItAGwFMBDSqkvRCSvr1NKVQKo9N5DNXN4KvTs2dNqX3755YHH+peC+adU0s7FuppLNQFg9OjRkZ9z7NixBX1dQ0ODjk+cOBF4XFVVldWurq4OPPaf//xns+d1sa4tcfPNN+vYP+X57rvv6njt2rWx5ZSEvFahiEgZGr8ZFimllnkvfyoi5V5/OYD6aFKkqLCu2cS6lo58VqEIgGcA1Cil5hhdVQDGe/F4AK+Enx5FhXXNJta1tOQzhXIlgLsBvC8i3zxUcAaAWQD+LiL3A9gLoLB/Y1JSWNdsYl1LSLMDuFLqbQBBE2jXhptOcswdzVauXBl4nP8JPK+99lpkOUXJ5brecsstVvuRRx7Rca6HGvt973vf03FLlv/Nnz/fan/00UeBxy5dulTH27Zty/schXK5rrmcffbZVnvkyJGBxy5ZskTH/m0IsoZ3YhIROYoDOBGRo0Sp+FYKpXlZknlH2/Tp0wOPGzBggNXOtdwrzZRS+a0ry0Oa61pqslpX/9TYW2+9peP6entBzZ133qnjDO3WuFEp1c//Iq/AiYgcxQGciMhRHMCJiBxVsk/kMXczA4AHH3wwoUyIqDn+pyANGjQooUzShVfgRESO4gBOROSokp1Cueqqq6x2mzZtAo81dxg8fPhwZDkREbUEr8CJiBzFAZyIyFEcwImIHFWyc+C5bN682Wpfe+3JTdwOHjwYdzpERE3iFTgRkaM4gBMROYq7EZaorO5aV+pY18ziboRERFnCAZyIyFEcwImIHBX3MsIDAPYAOM+L06AUc+nW/CEtwrrmxrqGp1RzabK2sf4RU59UpLqpCfkkMJfwpCl/5hKeNOXPXGycQiEichQHcCIiRyU1gFcmdN6mMJfwpCl/5hKeNOXPXAyJzIETEVHxOIVCROQoDuBERI6KdQAXkeEisl1EdorItDjP7Z1/vojUi8gW47V2IrJKRHZ4n9vGkEdXEVkjIjUislVEJieVSxhYVyuXzNSWdbVySWVdYxvARaQVgLkARgCoAHCHiFTEdX7PAgDDfa9NA7BaKXUBgNVeO2oNAKYopS4CMBDARO//RRK5FIV1PUUmasu6niKddVVKxfIB4AoAK4z2dADT4zq/cd7uALYY7e0Ayr24HMD2BHJ6BcDQNOTCurK2rKs7dY1zCqUzgH1Gu9Z7LWmdlFJ1AOB97hjnyUWkO4C+AN5JOpcCsa4BHK8t6xogTXWNcwBvap/ikl7DKCJtACwF8JBS6ouk8ykQ69qEDNSWdW1C2uoa5wBeC6Cr0e4C4OMYzx/kUxEpBwDvc30cJxWRMjR+IyxSSi1LMpcisa4+Gakt6+qTxrrGOYBvAHCBiJwvIq0BjANQFeP5g1QBGO/F49E4txUpEREAzwCoUUrNSTKXELCuhgzVlnU1pLauMU/8jwTwIYBdAGYm8IeHxQDqABxD4xXG/QDao/Gvxzu8z+1iyGMwGv85+h6ATd7HyCRyYV1ZW9bV3bryVnoiIkfxTkwiIkdxACciclRRA3jSt9pSNFjX7GJtM6aISf1WaPzjRg8ArQFsBlDRzNcofqTjg3XN5keYP7NJ/7fww/rY31SNirkCHwBgp1Jqt1LqKIDnAYwu4v0oHVjX7GJt3bWnqReLGcDzutVWRCaISLWIVBdxLooP65pdzdaWdXXL6UV8bV632iqlKuE9ekhETumn1GFds6vZ2rKubinmCjytt9pScVjX7GJtM6aYATytt9pScVjX7GJtM6bgKRSlVIOITAKwAo1/3Z6vlNoaWmaUCNY1u1jb7In1VnrOqaWHUqqp+dCCsK7pwbpm1kalVD//i7wTk4jIURzAiYgcxQGciMhRHMCJiBzFAZyIyFEcwImIHFXMrfSZdc4551jtP/zhDzp+4IEHrL6NGzda7bFjx+p4z54m958hIgoFr8CJiBzFAZyIyFEcwImIHMVb6ZvQq1cvq11TUxN47Gmn2b8Df/7zn+t47ty54SYWoqzecn3ZZZdZ7WXLlum4e/fukZ//+uuvt9rm986+ffv8h4cuq3WNyqhRo3RcVWXv6zVp0iQdP/3001bf8ePHo03sVLyVnogoSziAExE5issIPR06dNDxwoULE8yEijFs2DCrfcYZZ8R6fvOf5ABw33336XjcuHGx5kKnat++vdV+6qmnAo998skndTx//nyr7+uvvw43sQLxCpyIyFEcwImIHMUBnIjIUSU7B24u9wOAMWPG6HjAgAEFv+/VV1+tY/8Sw82bN+t47dq1BZ+DbKeffvLbeOTIkQlmcurWCg8//LCO/Vs0fPnll7HkRCeZP58A0KVLl8BjFy9erOMjR45EllMxeAVOROQoDuBERI4q2SmUP/7xj1b7xIkTobzvLbfc0mQM2LsT3n777Vaf/5/elL8f/vCHOr7iiiusvtmzZ8eaS9u2ba12RUWFjs8++2yrj1Mo0fMvI505c2beX/vcc8/pOM471luCV+BERI7iAE5E5CgO4EREjiqp3QiXL1+u4xEjRlh9hc6B//e//7Xahw8f1nG3bt3yfp9WrVoVdP5CubxrXZ8+faz2m2++qWN/PS6//HIdm7WJipkLAAwePFjH5eXlVt/+/ftDP7/LdY1Cv372Bn4bNmwIPLahocFql5WVRZJTgbgbIRFRljQ7gIvIfBGpF5EtxmvtRGSViOzwPrfN9R6UPqxrdrG2pSOfZYQLADwJ4FnjtWkAViulZonINK/9aPjpFWfIkCFWu3fv3jr2T5nkO4Xi39h95cqVVvvzzz/X8Y9+9COrL9cSpp/97Gc6njdvXl65FGkBHK3rY489ZrXNOxyHDx9u9cUxbdKuXTsd+7/nwlqe2kIL4Ghtw3brrbfmfaz/Z9kFzV6BK6XWAjjoe3k0gG/2XF0IYAzIKaxrdrG2paPQG3k6KaXqAEApVSciHYMOFJEJACYUeB6KF+uaXXnVlnV1S+R3YiqlKgFUAtn4qzY1Yl2ziXV1S6ED+KciUu79Ji8HUB9mUsUwH1z7/PPPW33nnXdeXu9h3vIOAEuXLtXxr3/9a6vvq6++yvt9Jkw4eWFjPgEIsG/5PvPMM60+88kgx44dCzxfCFJb19tuu03H/h0Hd+7cqePq6urYcvqG+bcN/5y3uazwf//7X1wpNSW1tY2Sf/dBv6NHj+q4JbfZp0WhywirAIz34vEAXgknHUoY65pdrG0G5bOMcDGAdQB6i0itiNwPYBaAoSKyA8BQr00OYV2zi7UtHZm7E7NXr146rqmpCTzO/7CFNWvW6Nj/8NkDBw6EktuDDz6o4zlz5gTm4/9n+IUXXqjjXbt2hZKLa3fsvfDCCzr2Lw0z/7/GsQTTnKYDgPXr1+vYXFII2A9ZNr/HouJaXaMwaNAgHf/rX//Keexnn32mY3/tUoZ3YhIRZQkHcCIiR3EAJyJyVMk+kce/3Oy+++7TcVhz3n5VVVU6vuuuu6y+/v37R3JOV5177rlWe+DAgYHHxrT1gGYuBwXs5an+v7vEMe9Ntpb8LMX9vRM2XoETETmKAzgRkaMyPYXiXypo+sEPfhBjJo1ETq7w8ueWK9df/epXOr777rtDzyuN/A+j7dy5s44XL14cdzqWnj17BvZt2bIlsI/i4X+Ig8l/NyynUIiIKBEcwImIHMUBnIjIUZmbA//pT3+q44SehhJo1KhROu7bt6/VZ+bqz9ucAy8Vhw4dstqbNm3S8cUXX2z1mbdAHzzof45BODp2PLl9trkzot/bb78dyfkpmPngaAC48847A481n5gFALW1tZHkFBdegRMROYoDOBGRoziAExE5KnNz4OY8cxLMJ+1UVFRYfTNmzMjrPfbv32+1I34KTyp9/fXXVtvcRte/nezrr7+uY/82vfnq06eP1e7Ro4fVNreQzbUFc9r+7lIK2rdvb7Vz3VOxatWqqNOJFa/AiYgcxQGciMhRmZtCSZr5YNSJEyfm/XUfffSRjsePH2/17d27t+i8XPf444/r2NySAABuuOEGHRd6m71/B0r/NEm+D8ResGBBQeenwuVa1um/df6vf/1r1OnEilfgRESO4gBOROQoDuBERI7iHHiRli9fbrV79+5d0Pt88MEHOubt2Kfatm2bjn/84x9bfZdeeqmOe/XqVdD7L1myJGf/woULdex/mpLJv/yRotGlSxcd57p13n+rvP9JXK7jFTgRkaM4gBMROSpzUyi5nnpjGjFiRGBfZWWl1f7Od74TeKz/HIXeiZf0HaQuM3cqNOMw7d69O6/j/Hd08gk90Rg0aJCOc/2cv/zyy3GkkxhegRMROarZAVxEuorIGhGpEZGtIjLZe72diKwSkR3e57bRp0thYV2ziXUtLflcgTcAmKKUugjAQAATRaQCwDQAq5VSFwBY7bXJHaxrNrGuJaTZOXClVB2AOi8+JCI1ADoDGA3gGu+whQDeBPBoJFm2gPmU6dmzZwce99prr1ntXHPXLZnXzvfYp59+Ou/3jIJrdU2a+bcV/638pqTnvEulrv4dCE3mtgh/+tOf4kgnMS36I6aIdAfQF8A7ADp53yxQStWJSMeAr5kAYEJxaVKUWNdsYl2zL+8BXETaAFgK4CGl1Be5rkJMSqlKAJXeewRvpEyJYF2ziXUtDXkN4CJShsZvhkVKqWXey5+KSLn327wcQH1USbbEsmXLdDx16lSrz3zYQlTMhzHU1NRYfRMmnLywqaurizyX5rhU16SZuxPmeqBDGpRCXYcNGxbYZ+7e6X+IcdbkswpFADwDoEYpZT7upArAN/uejgfwSvjpUVRY12xiXUtLPlfgVwK4G8D7IvLNXRIzAMwC8HcRuR/AXgBjo0mRIsK6ZhPrWkLyWYXyNoCgCbRrw02H4sK6ZhPrWloydyv9nj17dDxu3Dirb8yYMTqePHlyJOd/4okndDx37txIzkHxO/PMMwP7uANh9MrKyqx2z549A489cuSIjrP+QHDeSk9E5CgO4EREjsrcFIpp7dq1ge2VK1dafeYSP//OgFVVVTr271ToX19rPpiBsuPee+/Vsf9Bub/97W/jTqfk+O9wNh/M4N8BcufOnbHklAa8AicichQHcCIiR3EAJyJyVKbnwHN54403craJTBs2bNDxnDlzrL41a9bEnU7JOX78uNWeOXOmjv1bG2zcuDGWnNKAV+BERI7iAE5E5CiJc2c1bk+ZHkqp/PYXzQPrmh6sa2ZtVEr187/IK3AiIkdxACcichQHcCIiR3EAJyJyFAdwIiJHcQAnInIUB3AiIkdxACcichQHcCIiR3EAJyJyVNy7ER4AsAfAeV6cBqWYS7eQ3491zY11DU+p5tJkbWPdC0WfVKS6qfv6k8BcwpOm/JlLeNKUP3OxcQqFiMhRHMCJiByV1ABe2fwhsWEu4UlT/swlPGnKn7kYEpkDJyKi4nEKhYjIURzAiYgcFesALiLDRWS7iOwUkWlxnts7/3wRqReRLcZr7URklYjs8D63jSGPriKyRkRqRGSriExOKpcwsK5WLpmpLetq5ZLKusY2gItIKwBzAYwAUAHgDhGpiOv8ngUAhvtemwZgtVLqAgCrvXbUGgBMUUpdBGAggIne/4skcikK63qKTNSWdT1FOuuqlIrlA8AVAFYY7ekApsd1fuO83QFsMdrbAZR7cTmA7Qnk9AqAoWnIhXVlbVlXd+oa5xRKZwD7jHat91rSOiml6gDA+9wxzpOLSHcAfQG8k3QuBWJdAzheW9Y1QJrqGucALk28VtJrGEWkDYClAB5SSn2RdD4FYl2bkIHasq5NSFtd4xzAawF0NdpdAHwc4/mDfCoi5QDgfa6P46QiUobGb4RFSqllSeZSJNbVJyO1ZV190ljXOAfwDQAuEJHzRaQ1gHEAqmI8f5AqAOO9eDwa57YiJSIC4BkANUqpOUnmEgLW1ZCh2rKuhtTWNeaJ/5EAPgSwC8DMBP7wsBhAHYBjaLzCuB9AezT+9XiH97ldDHkMRuM/R98DsMn7GJlELqwra8u6ultX3kpPROQo3olJROQoDuBERI7iAE5E5CgO4EREjuIATkTkKA7gRESO4gBOROSo/wdy5nkzb5VT6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/600], Loss: 0.2367\n",
      "Epoch [1/3], Step [200/600], Loss: 0.2989\n",
      "Epoch [1/3], Step [300/600], Loss: 0.2266\n",
      "Epoch [1/3], Step [400/600], Loss: 0.1305\n",
      "Epoch [1/3], Step [500/600], Loss: 0.2382\n",
      "Epoch [1/3], Step [600/600], Loss: 0.1387\n",
      "Epoch [2/3], Step [100/600], Loss: 0.1782\n",
      "Epoch [2/3], Step [200/600], Loss: 0.1191\n",
      "Epoch [2/3], Step [300/600], Loss: 0.1174\n",
      "Epoch [2/3], Step [400/600], Loss: 0.0630\n",
      "Epoch [2/3], Step [500/600], Loss: 0.1148\n",
      "Epoch [2/3], Step [600/600], Loss: 0.1066\n",
      "Epoch [3/3], Step [100/600], Loss: 0.0381\n",
      "Epoch [3/3], Step [200/600], Loss: 0.0898\n",
      "Epoch [3/3], Step [300/600], Loss: 0.0738\n",
      "Epoch [3/3], Step [400/600], Loss: 0.0337\n",
      "Epoch [3/3], Step [500/600], Loss: 0.0599\n",
      "Epoch [3/3], Step [600/600], Loss: 0.1015\n",
      "Accuracy of the network on the 10000 test images: 97.7 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device configuration\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = 784 # 28x28\n",
    "hidden_size = 500 \n",
    "num_classes = 10\n",
    "num_epochs = 3\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "examples = iter(test_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape )\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1) # subplot(nrows, ncols, index) index decide where the picture should be place.\n",
    "    plt.imshow(samples[i][0], cmap='gray') #samples[i][0], [0] means the first channel\n",
    "plt.show()\n",
    "# 100 is the batches of the samples, 1 means one channel(because these are black and white images), 28,28 means 28*28  \n",
    "\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) #nn.Linear(size of input, size of output )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        # no activation and no softmax at the end\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss() #applies softmax & NLLLoss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # origin shape: [100, 1, 28, 28]\n",
    "        # resized: [100, 784]\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1) #get the max of the outputs and along the dimension one\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./images/CNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./images/CNN2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./images/Maxpooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pooling not only reduce the cost of computing but also let the input become abstract, which will reduce the possibility of over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAB5CAYAAAAgYXpDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO19e5Bk51Xf7+vu2++e93v2MfuStHpLlmQZG2Ns49iGwkkVpOwQUCWu0j+kgBSVYIc/iONUAZUU5FGElAsITgowxBAsDAGMbMeAbFlrSZb2oX3P7szu7Lx7pnv6ffvLH+d895zZmdmd3R3tbMP3q5Km97u37/1efe8553cexloLDw8PD4/OQ2y3O+Dh4eHhcXvwD3APDw+PDoV/gHt4eHh0KPwD3MPDw6ND4R/gHh4eHh0K/wD38PDw6FDc0QPcGPNhY8xpY8w5Y8yndqpTHh4eHh43h7ldP3BjTBzAGQA/AGAawCsAPmGtPblz3fPw8PDw2AqJO/juMwDOWWsvAIAx5gsAPgZgywd4Npu1PT09d3BLDw8Pj79/mJmZWbDWDl7fficP8HEAU+rf0wDeeaMv9PT04Pnnn7+DW3p4eHj8/cNnPvOZS5u134kN3GzStsEeY4x53hhzzBhzrFKp3MHtPDw8PDw07uQBPg1gr/r3HgBXrz/JWvs5a+1T1tqnstnsHdzOw8PDw0PjTh7grwA4Yow5YIxJAvg4gBd2plseHh4eHjfDbdvArbUtY8y/APAXAOIAfstae+JWr5MvpAEA7WYzagvicQBAs9mK2oyhd00sFtDfuLLgtOm8WFuakqkUACAMQ7lGjK6RSOYAANWK3DNuQr6+XKOrrxsAkMkUorbFheX1Awjr0Udrm3wN6VurRX0L29KPIEHjs2yFMnFZhvYmXkHZbAYA0GhJf69eW1x3zqc/9a+iz3/2u/8VAHDuxLejtvf/o38GAHj83R+RvjV5zNwPfe9SqQQAiPNaAMBrr78MAJibuxa1XblCNEg+T3OUy3VHx9zcP/WOp6K2tTUyo2UyuajtkUceAwCceOXrAICTr70cHfvh534KAJBMpqO2ncig6a7x2c9+dsOxk+VfBAB0p45EbcU5Ui7bKEZtqTytS6F7GABg2jJXtkprapVVcXVtDQBQaVWjtnq1QR+aSQBAvkfmL9tLbbnuTNSWTNL+b1v5bTRqZbr+8ioAoFZRv5s4zVsQS0VtMUt9ixvZp+kk/zZSNIbllbXoWKlM122F5agtztdrya3wnsM/A43xQ49Hn9tt+nEaczPLq1l33rqV3uyrm1ziRqftDlznpGcyDWbDWVfOvbbtK98JiQlr7Z8B+LM7uYaHh4eHx+3hjh7gO4FImlJv5pDf1k31ejf8pgoSLCkkAjnf0vltK9dw0qSWwC1fL5Z0UryI7E7wVsIzDhzYAwBIZ8X1cXFx2d2A/kCuYbktpsV4Gx3c2Dc+Fo+J5JZMJjf0u9Egybter2Er1Op19bnGt5RrrK4s0b2M3AusCbgxa8m2q9AFYP0apNMkzY2MjkRt01cuAwCWizQv+/cdio61eAx9fUNRW3d3e8O9HAp8T6dxADKXWnLbXIrbOVi+ftzIHmvU6KdilEbiVrTVoLFUFkXKzSZp7PGU7IV2SPNnZVkQi9F1Q77YalEk3wZrdy2lnRZ6aG5igZoDS/cIUnnqo5EbhE3qm60rTZR/J0Ggfv78m4vz7yudkfNLrDW11V5otHiO4ltbYdf9Dm6Irddz3VrfaNn1adtR0DbT4t62fbXxupvu4du4vQ+l9/Dw8OhQ+Ae4h4eHR4di100oTs2KKdW0yWRdIhAVNslEX4t1TU0UtlkN1eYMZxdIJGSIzVaDD5H6lM2Jqt6olPl8UYOLM9RWmp2Ua1TmqD/dPdwfUW8jqmKdesbjM9IPp5xaVj+TqaSMJVRjcOc5k5Iyk1yPy9dm5I65AvdNTC7zfLypiFCnvm+mczpLS0K947NZIh7XFkpR24EDBwAAr772HQDA7Ozl6NgzT78LADA6Orbh+s4spBGkaT16hkalH2xe0lPqzC/rtFD+hzMltbTpJ8Omi7bMbWWthK0QT2T53jL2mMnxXzEtrMzQGFZL9DcL2cOrAX1ODimWj00W1speSCbZpMSmi0azIddfJLNX2BT32yCgsQdZ2af1GvepTeM0Ru6ZYVNOo6bmg00oaWW6q7b4vAaZS8L2iow93uB+a3Ke2hJJ+Y2+LVhHTsY2a7zBl/ivMl+Cf4f6ueCeG622zJEj9tHme27LLnPrWGdKvI1beAncw8PDo0Nx70jgSpxKsDSuSUknDaWYvNTubbUaSZpa6orz+VYRQM4NL2CJzEARhSz5BEoCnz7+OgCgubwQtdV7SLqNs/tjvS4uYZbf9KGR92LAb/qYknbi7u3vxqAEhDpLkJoACnlcQUJLO0qyA3DhgkTaNsBujzFx1ZtjslFLnkFApJfjoRyBBQBtlkaMGktplaSz6SmJ1xoY6KfesOZg2qIllIukrTjNB5B1TiRkbVdXiQBdK5PUl+/pjY4Z1qSsZv5YOlpeFpe+pWW6Ro3XcWZGNJKJiQkAwPnz56O2hdkNMWcRsgHPn5KIkmnaFxdPyRrUlmjM+0aoP/2D0se1Bq1xfUkWNzlGx7NJ2TPZDO1Jy5LeWlnmykm5oXKZtawJNBrSucAQSfzMEx+lcV7+S+ljeR4A0AxF2wwiAlJ+/jXWJMsV2h+NUPaJ+2kaJYUmnEuuck982xHdXovdm2is1xGUVrvqNWjuyxUhi2Nx0nBy+S71nebGW+0gNiPxb8c91kvgHh4eHh0K/wD38PDw6FDsugmlGZk4lErIvtAxpb840iuZcBFdYpJwREpb+T27CDiryc7oHmxeUeYBF7kZi4mqiT4i05I9A1FTLkufExkyMdSaQto5kqfVEDU4zaRJLLHRUTUiT5R5wBGaLRWF2uA5Simz0fVo1MRkYOMl7qNEL5bYDHT+rER5pboo6s82qR/ZnJBltRqZQlZXRZV+/Q2K7JydvRK1TV+h74QtOn+gpy86trZKJFyzKUnMnNquTUTTF08BAJavcVRnr8x3sUhmknpVTDMpXquTJyVz8TKfN8Q+6hcuXFDXINPM5EVpSya21o0jM1pTzqmxyWLygrQdGaf52zdCqnc6IXu4zVG5xQWJXqwyeZnOSmRvNeH83Om76byQh4OjtC8qK0JyF6doD6Ry8tM9dOAwAKAr2A8AKC/J/oszsRrE5RoB7/u22nfW0HcqFTIxmLiKHOYoTeTlGs06rXsiuRv5jWSeb2R2cGbXtjJzzp4js+j8lYtRW2hovUcnjkZtYwcfpO/yutx70Z0EL4F7eHh4dCh2XQIHR+u11Suu7lyp2vJ2LTP5ZphsGRqSaMBUhqTmdihSa4pJJx3xFGuQpJ5gCS4IREKNR6SMRF0OTpAEHledm50lCSnGEW6ZpEhMlRYRIynl4hU3JLUYJfHFYvTdBrtutZWW4PqrlAOkUtTvQEdRYn1Upk2elntaJmTzcrzBeTjefF3yjfUfIkm3zZJmbVX1Mc6kVlkk3zOnSYKt1lRa4JC+0+S5f+1VkXaGhmld4n1yfsASeFyTxW9NAgAMRwuuVEXCn6tNU9/KsrZJdi+dn1PS6sA++mBpnK3Wqlxjjj4bI3MWTyhN6zqYkPrWVpLe/CxrikqS7S7QIh0aI0k8pTjmEo9lcUly51ybprkcH90TtVXqTOqydmXXZNGuLZBWlW7LzzTPuWbay7JWF5ZJEzl/jNanVJJ5yXXTeS5fCgCkEzyGmNIYnDtgm2oGhDHREsCHAqvcellLNtmto4N3GpsL245h1QeZ2OTf7ewFSdF08tWXAKzXLDMF0iIWi/NRW7aX5qF7kGsotDchHbcZ/WlvJL6vSwNz63K+l8A9PDw8OhT+Ae7h4eHRodh1E0qSzQKh0h5qdfZ/VWRgNmCVO+F8xMX/M8GqRyYjanF8E8LPRW82OUKxrSKvDEex1Spyz1H2vw2Vf/n8VSItsznyVQ5TogPVWE3VqVJbZj1xSp94zKwT2prc05GYVkcBOlJ3o8ur3Lu+FH1ut5joTQsR5VLiri2KaSEx4ExVNL54SwjIkH2Qmw1RNcf20nzUazK3MU6hGmMf9ercbHRsqs7j61H35HTAiYRcY/Y0EZAjvWSyiinf3laN7lld0/7RHL0YijljefUMACA7zylee5S/NhOysUDmox26+VWmAnf9Jq9PW8UQrNF3q1W5hrOKHdxL85YKZP9dYx/44RHZC2U2sWl/apfStbuL9t/CwnR0bIL9kvfvFXPht9+g69qk+CwHbTYvstkrE8g9G2u010P1AyuWeVxt2WMpjoJ10bkxba5js1cQ6IRYdI14/m5W2drMTuEysemYEfp85i0ynUy9+a3o2MgI/W4PHjkctbXZjHv+osRSzM4Qod43QqmCw3UOEpv0bDNLznZwh+yol8A9PDw8OhQ3lcCNMb8F4IcAzFlrH+a2PgC/D2ACwCSAf2ytXd7qGjdCm6MjrSJU3FtFp4xNM8limRRMqCgyl25Tk5iOyGiG2uWIJVOWfOM6T0qT7lpREVohS3qXpyUS860z9JaeOEzfLaR1ThYi8Cp1kRaRJNIppgggF43YdgUVNEHC8xHTuRo4XajWBK6HbajCEhxhmhFvPCwsk5TWnJe+FUZZsu/i72ZknCneGvGUSMM9w5wjxIiEl3DnscR2+q/FndF0Ub97ehVhyMRSqKaoWaH+5kdJy1pqC5mUqNF3Q02cptw4hUBzxTSaTe5PTNzbchkiBkOVZ6YZ5WLZKIG7VL6Npsxpbzet30ifSOV79pHGUKxy9GJR+vjWBYr0TCtJeWSQJPVaS/qRztEaLBVp7tOK7P6BZ+4DAHz7tMzHCM9lRRHrS1Wa52zBRViKZtJiLSiZVhI150pZF6EYuRbSv3NJ5XYY0LHyqvw2jJsGo9RCqUWxq3DKeR8TkNknvyc69uQBihw+ev8DUdtykTTEP37hS1HbCrvAGuvmTf/27h2nwu1I4L8N4MPXtX0KwIvW2iMAXuR/e3h4eHjcRdxUArfWfsMYM3Fd88cAvI8/fx7A1wH83O10wCWtb68rfUb2QF02zfIbMJ7gYzFt42ZXqYa8Ja2TopTvkZPUs3GSiuJJcWWrlNmVrqykOjY6J5Ny3XzKuflxEE5LaQ6uAIC6p2VNQL+znUuXi6PIpMSdMZMmyadcEWnO8nykb5D5LWyqfrNUFAVgABg8TNe49JJIyOe+SRLVxDNk5wt6RMJqs++YzkAX2ySJfxijOa1w8MjygkiLh46QhJrQYgIvW60o7oZ59nfM95BkX6pIjpPKMmfHUxJQkGY7rSrIAVfOjNej0VbzwQE2bZWNUJckux4ha22hOt/GSUrrK8haje8lCe/sNJWYm5mWfs8X6bsZdX6Spe1qSeY5mXZaIf0OHjwgeWAGRsilNXlcStgd2Ef28N/9KwlEWVwmafGhB0mbGOmXCW/xOBNqT/b203nVusxBmyXNFEvv6ZSs+/e+m7JJ/vlfiTtekdfF8QXUOewgnG17u+fLiY5jGBig/dfsFm7HpJn/UllEXf6evi7RFM+cpPmtlGmcQVY0knXZDXcZt2sDH7bWzgAA/x26yfkeHh4eHjuMt53ENMY8b4w5Zow5VqncTcbaw8PD4+82bteNcNYYM2qtnTHGjAKY2+pEa+3nAHwOAMbGxjYoRJbdycxmkU5K7XPuTS4Re1OliUWU9F/OdylMq6pqfMjEVTZJqlKQlfdXX4Y+z86JGnzuCvGyeYip5fDhCQBAmbWoSlm42zSbZOKBqFuW35FGTbVT5V1V+K6cItJ4CC43CwCEjki5gTpZq4lJwtUP1UUyklxtvDAg5N6514loWy7RGI48MR4dyw+6YhPKPYuHoF2qTJZMBHMXFumeSVEvs100hrU1lQqWzTvXJsXtcaKfCKVsmvq2dFzcDpsBrV+qW8xHdXYpbK8LzXPVzN2/lNGqzYU81pnkNhaUiE6PCEK5fq6H+j2UE3W8XKd5e+sC7ZlAFe3o7qf+Fldk/8VdkYek9KO8SianwR767p5RiQR2EclLJTEHXeK1Wl6TeV6rUj+nLtHPcP7yVHTM5eAZGpbruiISKRVtWeKI20SGzi/XZH4W5omk/eiHnonaXnmN3B3PXZZcQDuDjW6Bt/Q9KLOfS3Eck3XJdhOJmUnLflqcY5OWcls2NSKVL5z+LgDg/ne8U67v3HTvAS7zdiXwFwA8x5+fA/ClG5zr4eHh4fE2YDtuhL8HIiwHjDHTAH4BwC8B+ANjzCcBXAbwo7fbAfcSS6SEQHD8QqiIhga/HUMmqbQLlJPvEimROC2/dXNWFSlgt8SWC0IwypeN38ir4WLUNP0G5U14dEwk0wPjxNRMHX8VAJDKiARuW3TPfI9I4E4SbCqStu4yDbKE1Vaujg2WUK1yk0yyhhE2t5Yamw3tLukyqKmAGL5nskvmY/whksAuvkUk2TdfEBPX+IFhHoucPzBM0svguBBtcZZ2Vi5TIE3vPnExtCyRVkvS79oKjXnhouTmeOcP0r0unqKCC8UVIUL7DtI9dWGJJu8FnavEuWbK2LUEzi6GVVUyK3ED2YW/qkv6dfXTGqxOi+Zw8gRdb22VzuvpFq0pEafPDRVgtbrMkrSR/dGfJ9+7Pb10jb68SMVTUzRHb05LbhM3pzqbo+E9ky8QGXxw7/7oWLNB2szMjPLyjZGk3lNQ5QA5qCcRlYWT/fr1vzlLfesVyT6Ton5nd1oK3ZHKZddrY4J4wmnriojkPaPLGe4dpj1+/vjfAgD2HLw/Opbv7uKvbSwycrexHS+UT2xx6AM73BcPDw8Pj1uAj8T08PDw6FDsei4U5wOtib8G+/zGkmISMayupFg1VcFsUVSmJqZCfjfVmqKTrZToui1O41paEZ/oZY5UrC4oH2T2JX69JGaVvr2HAAA5JoWCovjoNljt1KYfp/q3VV3INvuru+jFtaqYE3Jc+b2uzCVZ9s0NQ62yXY+NKWk1qdtm01MqL0venyFVsDBE5qv5SZW+9xpN8PKsqN6zk6SOD4/0S9+6SG1fXCQStXBEIg8bdRpDl8oR0lyhMTdW5F5/89I3AQD1Npkb9t4/HB2zTDy2Vd6OZJLuuVaReXNr6kwnOh0vYnQsrooUNG8Q1dqK/PPFJNIsk4mjXpPcMLZKezebTfBfMbk4X+ShPpUXh/fwakmZcnht6y6lr6p6X2zTPIdt+R30Fuj+RZVet2ecc6bsoUHvv09+S9NnqW8jA9KPUpXWaqGpIm95Ly4tk3mlr1vI7jMX5vkaYt55YILusboouW92BBv5402gq9JvYrpw5hH3jDDauYHnTZmxuro5hFTl51kq0t6qcqGQybNSPOSRp95Ft3l7CtXfErwE7uHh4dGh2HUJfInLXWkJPMbEY5BSUhe7xNWYjNOxUA3OPdJW+TK6OAJuakYkhPPXSIJMsxuhSap8KuzGOKjKimXXqO2ykobHJoggqnHhgEJFXLyKnAulraSCFud4SamZTudIGmoxARiLqShDV7BelXZLcN9SgbrIikiCANBuqzJaEQGqotNiG10LXV6SXC7gfogEWZyiogoFFXXZ3U9aR0xlYJybo/PqDbruymUV8ccuhi0l7axx9figIP1ttEkSHD7EyfMDFf1p6btxNUdO01KeYGi7cfHesTqKkv/WdbTvDTinGufFSVmlFXJWv26lwRRypCm0mYyOqXSRswsL3CbXTWfJBfHZx4QUL67Qnr0wSdpgZlAGdegwnff00w9FbVemyG3vI9+3L2o7fZ7IxWZI+2/pmuzXiaO0X+sNif6ce4vcR0stmYRChrSNqSskbQcJiSBNc16UfE60qycefRgAML96WymQAGzuKbitbH7qe5Ezg5pnW6O1CnjPhyo3TL3Gkamq2knSWQHUb2NulrTBMpeYO3X8O9Gx+x56jL8nGprTdtd132z4sOPwEriHh4dHh8I/wD08PDw6FLtuQpmeJdIkoetCOr9o5YfbZBIuzSp9qNSSOOtdOZXsqcmVqK+pSLFpF2XJ0ZyZtJAyrkbjgjJF7GWf3MGURLGtVun49FmqP5i+Kn6+fb1sGlE+3E2O9rQqeVKzHuN7sgkjJeqcSdDnhBETSpvTpjpVfTM0GqreY5zmQftOh6zcJZWpCjznjgBMqQjB4T4yJVVnhSi0llTNrmHJG9riIgVBne45dUaCcqcvsdmhIGMJDJ2/534hQgdGyBwwfZai+y6ek5qYex7YCwDoG1PFGHg+jCo6EDPOdMLnbBKV21Lz5+qhbuZZH7LJSqdPDUtsJmmIic2maE3rTNbGleknzsnABnpk74CLkiwVVT3GhCsCQmNaXRGTRAlkGnnXM0Lqfnl6kvpWVKYLNp30DdC6JJR96PIkXWNov+z1+z9EFdeLl8S/vLbEc8p+4Am1d0aHyPSzsiqzdW2WyPtD+yewFTSJHvnn70D4ok6w5tzWQ5UA7fKr3wAA5BM0prE9kmWr2aZ6pI2Wyn3LfRsclLVaq1Nk6jJHwS5Wz0THrlymlNKHVEraFpOjOzG+W4GXwD08PDw6FLsugU9wiaOUJjFZWA2NSNTVGklDvXmScpuqQEIanMp0VaTF754jyTumUmY2K0RillkSa2ZEanWRfDoitM4S09qCpO585cRxAMBgnqollJfORseuVEgqMQ3Jl4FeetMHKvIvFuOyaewLWVsV6bLMOU1aLZVO1hWguIFvlZMC6frsNqfvyX/bOo9JFKpGf5OBIv64UMWSKjFXW6O+VS/LdV1Ok2ceYGLOyHxfukLul61QXNiScXavq4hUuTpD11s4SZJpfVW5j5qNUZftiKjUroCujd0OVaSdi1pMp3W079Zz2SjR3McCyS/jpFpdM8S6EmMc6dmlSukNcmrS8aHBqK3Zoj373TPnorb999Ee21ugNLHDIxLlWq/Rfq31C3l48H5KkXp5SlxbC72kTQz20TiLqmxevEZ9vPCS7NPjaZLKn/6QFDoYPEp9b3BOm5nzUtptfIz285snxe320lUiQh99QiIUV64rUG/fJj87LePGGiR5F1UpusLYBABg+q3XAADN0svRsdLyJABg/4QkUA3ZjTedFk14LxfruDx/CgDQrspeePO1bwMA9h08FLU5AvTtGvNW8BK4h4eHR4fCP8A9PDw8OhS7bkJZ4grZqYT2+WY/7YTy/WXCrc7VtUPlCe6SQa3V5Pyr82TOGFBqcxTYyURksy4kFTjib0IlrnJ1OqtNIXuq10h1bOw/CAC41BLSLsdmncS8XLcZkh96WpFZ3QnqUx/XSwybMpaAx9VSibmqLgmT8m2+3rVUFVBHi80YRvk9OxOOqx1JJ7DZiEnXlkqqlR8j1d5elhvNXyPirKCiIutNGnO2j8ih+KKMfaCHTTlpMS2kM0weVqRvV7mC+wL77448LmaHnglOx6v6HWfmykLmyFXPcWOwaixRcOO6Sklbyy6WSc9ksDHiz6iUsQGbcrqYvB4bEmIszWMOFbmMgPZRr/L1nuNEX2NcJL2RFTNgZZZMIdWSJJGqhHSPni4xtaRydI9UgkxQXTlJYbvI5sLRPXL+AptYXvrC/4vaBu6nqjvf+9H3AADqTVmfFpsmjx4Ws1eLa7dOXxGzSqH/PmhoEt1Vprp5llie800cwu0mftUtjsbN9ggpHnTTXiwMU3+mXvnT6FhsmpwP5q+JySUWpz2pnx/3HyHzyNwS7eczF8QZ4twpSmQ3denpqO3g4aPUHxXhezcITS+Be3h4eHQodl0CD/gdopPtV1naCqx0L8bRfCUmJdvqTdfgepANI1JaKkPfnS9Kvodedmcb6yVp5PSkvIX3jZJ0sb8gZGqJo7DaOYm4Ks3Rm7ifXZO61LEGM1zaBau2QhJCpaWkkYDOa7IbGlTK24Cj3loqKmxpkYjCrqy4sGW61pcAb9WEbHTaiUmIxB6yNqFribriGC2OIEwkROqJp6lvo/dJRN5br1A/5qZFquztJ0nz3BUik+ZX5J5zV4kMzGalbWwPSddT0yJpzi+RxJjkYgJ9e0T6C5mMNG2dB8b1c6P8EY8kZeWWGnPah5yXSGwtHQ2PkjTXWBHJN8FupkZVa+/j/CLjPeSil0/LOCtcqb7ekj25lOf5GBUJuY8jO6srnB9HjTPTTfu1WhJCe3iQ1qDdKyQme42izXusd1DWuPQGE+WqAEQ3awyjYyNR27HXKZXv8R6KODz6rndEx+ZfIQK0V83fGrsUltekbwURggFcF+26XW5vG7lQ9CHLWmw8ptxj+dmQztD87Xng8ehQ8zTN3+WzQiT3jpCrqq4pk+Pf2qH9pJmsVWWck1OkdZz8rkRn7j9whLqvB+2u9zYK4l4C9/Dw8OhQ7LoEfpDdpmrKvNtgW2ghp6uws32ZJciYels2m/T2bSjppbRIbn4X51WVdA4YObiP7NerSxIM8dAg9aOxJi5YrgSbSoqIuSuTAICJh5+l85UddnaJ3sz7hvbIF2oceKRsuJWApM+1Emc8a4g2kUqSxFvoGpCRs828XBHJLSOeZQCAdl00gbUqjUuXQwsyPJeKa2i4BPZsb4zHZP6ifCNDct3HPkiuVzNnpb9L8ySZfOe7FOiQSsiaWQ7EWigKhzBxhDQXGxfJPlEjN837nhnnsUm/nTuglmzaLCo1mjIfzmQaa28Ud1xQjw7u0blSrkfIxlajZL0kB+mMD4v72Z4ecq+b6KGxzMwIHxKL07wknhTb81CK1jQZyr5r1mm/JZecDV80k1SeRNpcQWzLMXarrC7Jvq5wObTZq3TdriHhHAq9dN2Lsyq7Jai/oyMyV48+REb4k9+kdexS3NH4k3T/1Tcmo7Y076P6DdwxbwvbuJxeYRNjrmZd+km3n5njURxMGTSuk6ckMGekQfs+k5Q9WWHtu1Wn9ejpUtovS+dvvvZK1PbYU8Qd7N0vQUMtzkpqdjMXijFmrzHma8aYU8aYE8aYn+b2PmPMV4wxZ/lv782u5eHh4eGxc9iOCaUF4GettUcBPAvgJ40xDwL4FIAXrbVHALzI//bw8PDwuEvYTkm1GQAz/LlkjDkFYBzAx0C1MgHg8wC+DuDnbrUDleYODZgAAB4ISURBVBiRcWvKbQkmxZ1T6RrZT869cXQejCYXebDKBFDIkqo01COqYL6PiMoVVp+7BsS17zLnoNAqYchRkZmcqFbOLaw4fQIAMDUjRGidI7oWU6Ii51jN7s2LaSEb0CgKBbKDlOsqRwe72RXSorI5U0siJfksxHhAeOTBj0SfV4ucLrQsfVteITK33lDfTLCbH5tabEITf1x4QUc7BtTPA4el5mLQJjerYTa1rK6IuWStQt9NxWUsw71cCCMtskPfISJAY3m6VlV53sWZaE3ElTnNEZVq97p6oS02ueiUvq46eUyr2VtbUBByZG9MRQInuO5kOqWIcnZ3navQdXv2CCk4y2aV3nGxdbl1zKWEgC4tkPmve5jmaqUog28nOGI3pkh05uQTKYn2Heqj43kmlKfPXY2OZXJcL1PVaV1aovEtFlWhkn4yX+Xy1O+3vnUqOmY4LfDEY0eituLrZIJotCVCcTu4WbrY7RSjj6mLRHUpQ1UwJaD91mqs8F8xi9oBMqMuX/1W1NazSudV0/LbWFkic1jIJHR5Rdxj+7gm5qyKeH3jdSI0x/dNyL3YlGMcO6rGtplZZTtjvx63RGIaYyYAPAHgZQDD/HB3D/mhLb7zvDHmmDHmWKVS2ewUDw8PD4/bwLZJTGNMHsAfAvgZa+3qdp3UrbWfA/A5ABgbG9vw/g0b7KqnijGAychEr/glOem2xcRiPCZSXYIl9pYiqRLsQjSwRwjFdI5yHdTK5OK1WpY36GqZ3uAplaS9XqU3cm1RpMosZyjsjnMAUkak88VZIjHnrWgHyS6SOHUR7CoLcTUu1wRFvlZCVw5NlsZpG1UV8KO8AQEAPQV5f3bnmJBtytjLJdIKVldE6qpwBrfy2hIfk4xuIUe/pHPSj2yOpLj+vEh/q3ma8xpnbysot8pkwNW7lctnrUJzv6ZiqCxL5c0auWhaJVo7kioe6AAk+qMlauc+aF0FBcVyhy6BiZo0275uAhUaTCzmAlnbVEjXdXMAALk+Ou60EOyRY8PjlCOkOC/znU3QumTzQheZGkmy9Tr1x/TJvm6W2BWxJJJhTzdpkcmYSPbFWbp/1whJyAcflGPLvCfHRkVSnmX3zuKK0oL6qe3AAXKbW16SjIln//okX1/WPRin36Y9I66WN8JO0nja3Q/82whVFtEE363lMlQmReNJDtPvd3z4vFxjjcZaUe68s7xuTqMsrQm53ObtOaQCty6efhMAcG3qiahtdB8Rmq4UorHrRPAdwbYkcGNMAHp4/4619o+4edYYM8rHRwHMbfV9Dw8PD4+dx3a8UAyA3wRwylr7K+rQCwCe48/PAfjSznfPw8PDw2MrbMeE8m4APw7gTWPM69z2bwD8EoA/MMZ8EsBlAD96Ox0YGeAUjlZ8NV0kVUxFvTVjdNy41KpxlRqUVZRKTfTyXjZdxJUZxiXXn50hdbxUEsLIJbKvKN2+m80lXTnp2+IqmRlOscp03xFJKZnnghLzs6I2O21v/7iYM1psMlmukHqWVlGXNWc60eYgVv1byl8cot0DANoNMTGkC9xf5ZPdx+p7b9dY1NZkQrbCvu/FkihRyyXK4bJSlkjWJJNqaUXqxhwb2HZ1IaVPuRytUVNFia6sst+zMj2ZMn1uh9TfdiDmNFcbI7TaXEKfW7rGJeukKd4zWlu1IbW1lbzSjt2ATTOcWlVdxKV/yaaUiSOkseQHyWQR5IRkblZpXrrVHnamO1SkCEjAY05xXcZQzW0zRr+N/J7RqG1tjnL8xK0i3JrUj/lJUvP3H5G91t1F363W1D3zZKq6Ni3z3KzSNWLsEHDkiOQEOvbyWwCAuVfF7LD/+8hUkJkRs9tOwPGTNyL01q1cjDaIUfVco5gAJs+zXTKnSS4MkxwWwnnxEuVHCUak9mjfXvrNLdVOAwCe+f73RseuzlBBB2eeAoAG52S5cOr1qG1sz8H1g3kb3MG344XyNze49Qd2tjseHh4eHtvFrkdi7p8giTCmRLdqlaRgq6LqwgZJNxVLUkPNqvJiTTovviYEWqpF0qVVLkchu641mAwc7Bfp/Jn3UmaxqcuTUduVC1Ta6/D9ImUfP07uVQ1O8J5SZdyGRuh6JVXuKmDpOZNSUit/JeBI07giOEOWvHPpjVXB63WROKdL64uBLS2JhDWYYqlORdM1ORK0rQjCGEe35pMkceb7RSoZrHG06Jpct73KJO01iQI0LHknOFIxriTlVoukxLhmXJ1Hlap63qrSuII2aQnNrIyzyQUlEmlFQHIUYJBQbawJ1BvU7xVVJMMaNx+yP4L01lu/GbJ755pIuYUCfXe5KGRWL2eMTLHLp2ko9YMJ8GRShcwuU1t7RdbONGhta5wZs60iZfv7aD9Vrl2Ty/Ln0X0SnTn4JEl6p06QZPjmt0VSHjtAEngyK5G9QwM097mCkPPlJdK4lstc2q0kBP8IF3S4OiXa2MgiaRPJvZI5EnK5O8a2qtMDkUtpPClrG4no7DlglQeBc0vNDUnEZOUSSdmNmBR0OPwoRVYGOZq/R558Z3SsybmXLp+XIhnn3jwGACjPSznAmYukuYweJsk+1C68vMxW5+zZepRbwudC8fDw8OhQ+Ae4h4eHR4di100oSVZvazXxU+WgN6QC6Z5l1ScNJqR0NBZXVQ8zKsWsK6AQaH9fUlKuHCff2w9/8P3RkaPveBgAsHhEos3+ZIUcaw4dPiz3atB9Q65n+dhjj0bHiqyGLs7OyvlMWCazQma14MhTOtZQtEw+R/2uN2Q+GnyvWLC173JRJTcK2JzR0yP+xi4qLKHMGSZJc+qSWrXbWtUks0C34pZrbFY5v3wpagt5fPUm9TepiNMmJyWDiiTs6qbxXV0WU8TiGq3bfk5slimIqtlsk17eqKi15Q2Sych1XcrYBpuK4jpdbIxNFoq4dNlBU5tMaa1G81JR9VQvrNH8DnaL6r1S5+IilyZpbMq/O5Uls0OrLNfoylFbMi17IazTHLXZXFhcUGRjg0wdRpkGx8don7YVgXv5DK1H0KI1iCuu+9JJisosLsmcZnvp/omsmLEKeVrvvXtpbt86Jfspk6Rjvb1C0l45Tvcce1pMOVvvzp3FelMDF/BYF+bIqZOjf6tUzq6ISV5MSsPdNA9zRTE9lZhk38dpYvVvI8HxAYceeCRqG91DZtapc8ejtqlLZELJsENF14CYKNscm7B+LLduRPESuIeHh0eHYtcl8BaXr9JvOBdhpxPwNznXhWtcT4xx5JVyV4sxcRFX76ggxZIPX3/fPsnpkWG3xK60RFcN9ZFEtW9Y3LKSNbr/CifsP3BIJLL6MJFOU5OTUdu1BSKDRkekHFWQoX6s1UliSigCN8WEYkNJ4PG4I19FippdknwXAICWHFtdoGOr83JOoZDna6nISm7r6yXJsFxT5CEzraYl67K4QC5j1xZFek6kuCBHm0ljnZOCQ9YGBiSCr7ePpLiSKl/10nFKJzvH13/6napkW8gV4q0iIDlNbdgSiaXB43fpS0JFDAdMgNbV/qjzXKaEt1L95ir2Kh3vtSmay7+eFwJy4hMfBQB09dNF9NgTnJp0RBHDiRztrcKA7KdVjngsdBMZ2FbugYbzv8zPCYmZzvA6ajL6FFVfj7HoXWmfiI4NDdH5s9eEYZy8QDly5qdkHde42MTRx+j8gX4hwM+fpT38DlWB/jVes/wekWR74wdxz4OfC6FyQ7ZJ2pO1ZSEg46NEPOa5cIquNm+bvHeVwJxk98+Dj0ghjJVZWvvLF8jx4ZByTMjk3HVlo95ORXsvgXt4eHh0KPwD3MPDw6NDsesmlAabETQJkWAVuV4XdTIed5XISeVoNMXE4FKNBsrv2ZV3NOodFecosyonm1otSRTZ4BiZOGxM+pHOkmpllNnBEWcFruDjfNYBoMnZFntUvcomq/mZjOjqMR6Ly6eUTgmpFeP+ZtJCGDnFKmxtrWItqyrblsnRnm7pR32VSKmlokTf9fMY7jtMNQGzGZXmtEKmgtV5ue4q12bUlYF6uDRQ3JB6qNclzWRdoSBMaCKgNlfhBgCGCjS/hS5a296MpPld4CRCJVXFvs0+3qm4zEcszr7kWa6AFFcEJ/tnq/KUaLuo1k1MKIbJ854uuefsKbrXlKrC/sd/8iIA4PvfR2rzvj1iQsjGybRQC2XsjWUyl1yYlFSmK5zGON9FZqasivpd5WRnZbVP81wZJqZqQF67QiaW4VFKaGbisucHudx9/6gMPmQSuHFuMmrLMYnp9t3EQenHiRNkfpmdkwjjgW6auBOvnY7a3vPUU7jXEZXcVJGbzTiN9drCxahtzKWvdr9VZUp0ifx0jEmUYUs19Y1QjEuDyeUzx49Fxx59x/fSteKyjvZGOY63gJfAPTw8PDoUuy6BV7kYZkxJU64Oon6buUhNw5F+OiF6nJmrRFakrpBZrGZTiCXnQtTD6UsvXxVyaO8hkp50ScUY572orElqTXA/XZRcvSISZ5nTf6YUKTmxj3JKhOpVmWApP82kRmNdTg8usqAitFyul3RG8nBcj+6RiejzDEeWmSVxZ2wzWdw0qj4g50CZ55wOQ/0iWV85Ty5VyyqqtNQgDUcTLxnWHixHiVbqMh8xTgE7MyvzbCmBJbLd4nL36EPkgvXQA/Q3iEuOmlyZJNPltKzBapFc7ZaX5bqwJFGHyzTRcVEm0N3FUnlLRe/eIB1yJkvnJY3cs1mhdU+lRHK7Ok+pVF98iea5Oyuaw94h0mpGBiVS0RV0qFUlyjFfIInXEcoubTINiQtipGTNFuZ4LIr0D1jbuDpNknJXlyoAUab1S6io1fFx6metJJHIi/M0l40KnXf0qFRyTyRIsn/p65Ln48HDRN6f4zwpnQKXS8kqLcUOTgAA5l55M2qLcaGW8X3OrVidv0nhTreb9LZyrqFje8jVcmZKnArOnyVi8/DRh+W6t1HRwUvgHh4eHh2KXZfA05HNTwVqcHBDWhVXcDbwKD1dSuzdMU7q3lK+Yw2uCJ1Q10iwbXNkmOyTp89IZeqqq+Ol3oINtmvNzYndc2iAJMeWK1JQE2mxyjbwpNIEBti1sKpyS7QCOp50bkVxlQ8hCjpQGkZso9R1PR58j5RUywyQlLs0KUEF5TnKQGcCkUIPHSa3sOFeGtPSnEgIa6x1LKliApOzLI0HMr5Gg91A2S2vuKoMzRzw0GzLHPWzBrVXFdoYHiUtJeD1CUO5fneebOy5lNju692cp2VQrlFcIRv/wgLZaStF0QRWeQhK0UHI+02lw4nQlSdJdu7EsvoC59xIiPTltmQqRX2cUOW0wAFfC0uSPyTLPMh9R5+M2kbH6Du5Aq1B2JD1aXDxC20bbVlXnEJ+ui12a6uUaOxrZXEZXOTMeVo0bHOZvGxBNLpimfrGCSqRionU//BRsqNPX5Igo0Xez2N9wtV0AqKftyrJOL1EPFZG/Q73je3j81zSklu/l7uay2/08BNPR8euTE8CUM8RAHEVBLddeAncw8PDo0PhH+AeHh4eHYqbmlCMMWkA3wCQ4vO/aK39BWPMAQBfANAH4FUAP26tzvG6PYSc9rWuCCaX9jNcV1h8fe4Aq9KWxrh+o66RmGD9VpOdLa5871yxMio/SW2NVPSUqvyeCMhMs1qVvvWEpA5VWddsV8RkUOGk7oEy77hyl65qOiA1LhtMerYVKZhgNUpH2jncKFJrpSwmg/McdTd9RUi4IU57e3RCzA7ZDKnJqytcp7IkZgpr6fy1qtxziVXN3hFlvmLzTo3nVteztEyuFXqE3Btk16qkcvl043IVxnU1eLdmJiHqfpJroAaBEKG5LJGFA/20Bg2VurPIdUBXS5Lfo1bdOnNH1tBcxiqyP1y0bAjti8jRqhxxOjYqboQPHeIcOcrVbGWFzCk9PUJsptNkugg43XAiI+lnWxlHGsstLbtrxpT7WZPNVy2ul1kpi9thZY0+x5V7bLVF6zxbFhfA1CDthQKbrK5MCzmZAO3/hx7aF7V99xgRfnv3bmKDcn3V0Yub7N3t1tW9EW75unwsbIo9LcWur/t7dBUQWmdJ96rMl9vttlm/rxMqpfSBgw9QP/TFbsNMsx0JvA7g/dbaxwA8DuDDxphnAfwygF+11h4BsAzgk7d+ew8PDw+P28V2KvJYAM63KeD/LID3A/gn3P55AP8WwK/fagdi3AVVFyF6qwbKZUvetOxYr16IljN71WoicQYsyVo1ROf+1pWnN+HhgyJRNNnZvqWCZeJcdMC5DAJAioNTnEaQUGSSyyQYU2RIgrMoukTyus3lf6mqwBiXiTFIq7G7pPXB1st19EEpB9VmaeHFr341arvYICl0Yp+UVLs4Sa5SAUsIbRWEs8qkblm5STq40mqASHZVzjOiXaxckEJ3r+TLSLPUH65zmTL8f84ip6RWRzzrKvINVvTaaq1MjKTVFJPWyUCOZZK0fr1K8q1xOsL1ZTF4fCW6Z0ZlUUy6snbQ7nsk//T2kyZwevJUdOzAfnI/GxkUqdwF32SyouVls5xHhd1HoUvHsdYRUyX34gHNn5a8qrxpLNfZ6+6V/Cs9nMcnp9IuXrpM0vPcmriZJpivvbTMWla/aDzf/z4qbnBFlRD70AepbVJpeUuiwFF/biKBO9yqJH6rkv26c5y2p/ZYoZ9cVauTMqtpF9QWafUqxeP2RXDuz0ZtL+S9fqdKyHar0se5HuYcgK8AOA+gaK2jxDENYHyL7z5vjDlmjDlWqVQ2O8XDw8PD4zawrQe4tTa01j4OYA+AZwAc3ey0Lb77OWvtU9bap7LZrQNRPDw8PDxuDbfkB26tLRpjvg7gWQA9xpgES+F7AFy94Ze3gOEiBaGqqu5UdJ1iNor+a7tIRel6gwnFall8rdtMGKy/BhdjYB/xlHp9tZj0ClSOjv1jZBJJKd/fFOdpcZdNp+WlFGdzSkuldg35s1bjnIrn0qK2M0LoOVNLTOVfcdYGnafleuiCGI88QtFdP/4T/zRq+8V/91kAwJlJUZvRJMuYS5ubUkUQrs2TalxryPzlclyVXhUkiCgeXpemIofSnBS/q0tMBlFE7Tq/ZDYB8F+zqbqtik24a6joQpcC1kWthooYNkxGBzFtiqDjm9VUb6zQtXLdKnoxQWr2orITZNgsMdRHyufq2lx07Oz5kwCAkWGpp5rJEkHYasscxXjPJNi3vt4QVT3glLS6EIUzj1WrYrpwfuA5Z5pR2yTNeyyh9vVr56iW47selNSnrTW6x4t/8bcAgO5eMTf95VfJ5GJD2WPvfPYxAMADR4UUf+k7qubjPQAhxzf+9pLqNxfjectnVN1arkMbeSHsAOGqsRMELrANCdwYM2iM6eHPGQAfBHAKwNcA/Aif9hyAL+1Ijzw8PDw8toXtSOCjAD5vyBIfA/AH1tovG2NOAviCMebfA3gNwG/eTgdsjMlAJU0lOFNYXEkeLrdJjbPTJRUx0OTIzZiW6ljKbluRdkIuvdaqEwmm316uqnVSzQhXWkKzKVJRgsmyHOeu0NnEXKRkXJGYrhScjqF0EmeCmdukqtAeYynRtvVYqN9ZXXn7OpTLkkOjXCJG6sjBiahtYISIrVdPSsa1vT10Xyd5LFQkYnKtTn1MZiQizyachCWjYa/KqN9ag3H5XLR7Z5RVUklFEanbchqSur77rIkrFjHbm+SOiDGxaFQ+Gpd9L2woMsklp1HRmQ6tAdofB8aE5K7V2PVuQbJP1ldoX5RYGr7/oBDJZ7m01tCI5A85MkEluEKdP4fd/JIup4wiMUN296tWRLN0h3VBkwwTbtksaVImLvPXlaG5/dtXX4raxvqIVH7s4fdGbUvLlPtm/OQ5AMCrb5yLju0boesPjgqZ/6df/hoA4Ac+8gEItt6fO4ntSq+bnRdySsriVRUhW6b1S6h9aq6TbTeT4u8FbMcL5Q0AT2zSfgFkD/fw8PDw2AX4SEwPDw+PDsWuJ7MyrCLHFLHT4HSbyWRSnUkqTFT1XKnZAROhJqHMJY7Maqk6j631dTV1BGR1jdNu1lWtwdjG91uZ03PWGxy5mRTzR7PBFeiV/7ozDwTqPGd2cX2MBdpHnFVjKyqym4fN+uNw8vi3pR+u0nmoKpEz4XZ5WhISjRaG+TwyBSyXxYTSYNuIacs1XM76liIqG0x8ur4lk8r0w8R0PL6x3+vI5egvm0ZUffMwUl2VWcqtn5Y/XNSbu65O68nmkrhK4o/2JrYTRtBHx5YaQhQmUlwkY1TOa/QSge0SldWNXPO+I+So9cab34za8hkiy3JZibYMqzT3FmSa0z7fZSblXZwDoAjthEr0xnOfzZK5K6+KQpy4SEUEvntcUqW+8/EPAgAuTYspp9Wi+w6Mk//6zLLsycExImkH9gqxefkKzfPcwg38u/V+vUEitjvBdswZmxHmpaL8Dmqr9LlZETOkWSZCOs1pj3c6ktRd707NMV4C9/Dw8OhQmNuphHy7GBsbs88///xdu5+Hh4fH3wV85jOf+Y61dkPNOi+Be3h4eHQo/APcw8PDo0PhH+AeHh4eHQr/APfw8PDoUNxVEtMYMw9gDcDCzc69xzGAzh5Dp/cf6PwxdHr/gc4fQyf1f7+1dvD6xrv6AAcAY8yxzdjUTkKnj6HT+w90/hg6vf9A54+h0/sPeBOKh4eHR8fCP8A9PDw8OhS78QD/3C7cc6fR6WPo9P4DnT+GTu8/0Plj6PT+330buIeHh4fHzsCbUDw8PDw6FHf1AW6M+bAx5rQx5pwx5lN38963A2PMXmPM14wxp4wxJ4wxP83tfcaYrxhjzvLf3t3u643ARalfM8Z8mf99wBjzMvf/940xyZtdYzdhjOkxxnzRGPMWr8W7OnAN/iXvoePGmN8zxqTv5XUwxvyWMWbOGHNctW0654bwX/h3/YYx5snd67lgizH8B95Hbxhj/o+rNsbHPs1jOG2M+Qe70+tbw117gHNFn18D8BEADwL4hDHmwbt1/9tEC8DPWmuPguqA/iT3+VMAXrTWHgHwIv/7XsZPg8rgOfwygF/l/i8D+OSu9Gr7+M8A/txa+wCAx0Bj6Zg1MMaMA/gpAE9Zax8GEAfwcdzb6/DbAD58XdtWc/4RAEf4v+cB/Ppd6uPN8NvYOIavAHjYWvsogDMAPg0A/Lv+OICH+Dv/jZ9Z9zTupgT+DIBz1toL1toGgC8A+NhdvP8tw1o7Y619lT+XQA+OcVC/P8+nfR7AP9ydHt4cxpg9AH4QwG/wvw2A9wP4Ip9yr/e/C8B7wSX7rLUNa20RHbQGjASAjDEmASALYAb38DpYa78BYOm65q3m/GMA/qclfAtU8HwUu4zNxmCt/UsuxA4A3wIVZAdoDF+w1tattRcBnEMHVBy7mw/wcQBT6t/T3NYRMMZMgErLvQxg2Fo7A9BDHsDQ7vXspvhPAP41pJBlP4Ci2sT3+jocBDAP4H+wGeg3jDE5dNAaWGuvAPiPAC6DHtwrAL6DzloHYOs579Tf9j8H8H/5c0eO4W4+wDcrPdERLjDGmDyAPwTwM9ba1Zudf6/AGPNDAOastd/RzZucei+vQwLAkwB+3Vr7BCgVwz1rLtkMbCv+GIADAMYA5EBmh+txL6/DjdBpewrGmJ8HmUh/xzVtcto9PQbg7j7ApwHsVf/eA+DqXbz/bcEYE4Ae3r9jrf0jbp51KiL/ndut/t0E7wbww8aYSZDJ6v0gibyHVXng3l+HaQDT1tqX+d9fBD3QO2UNAOCDAC5aa+ettU0AfwTge9BZ6wBsPecd9ds2xjwH4IcA/JgVP+qOGoPD3XyAvwLgCDPvSRBh8MJdvP8tg+3FvwnglLX2V9ShFwA8x5+fA/Clu9237cBa+2lr7R5r7QRovr9qrf0xAF8D8CN82j3bfwCw1l4DMGWMuZ+bPgDgJDpkDRiXATxrjMnynnJj6Jh1YGw15y8A+An2RnkWwIoztdxrMMZ8GMDPAfhha21FHXoBwMeNMSljzAEQIfvtza5xT8Fae9f+A/BREPN7HsDP381732Z/3wNSo94A8Dr/91GQHflFAGf5b99u93UbY3kfgC/z54OgzXkOwP8GkNrt/t2k748DOMbr8McAejttDQB8BsBbAI4D+F8AUvfyOgD4PZC9vgmSTj+51ZyDzA+/xr/rN0HeNvfqGM6BbN3u9/zf1fk/z2M4DeAju93/7fznIzE9PDw8OhQ+EtPDw8OjQ+Ef4B4eHh4dCv8A9/Dw8OhQ+Ae4h4eHR4fCP8A9PDw8OhT+Ae7h4eHRofAPcA8PD48OhX+Ae3h4eHQo/j/ycgcNDqM2lAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 32, 32])\n",
      "torch.Size([4, 6, 28, 28])\n",
      "torch.Size([4, 6, 14, 14])\n",
      "torch.Size([4, 16, 10, 10])\n",
      "torch.Size([4, 16, 5, 5])\n",
      "Epoch [1/5], Step [2000/12500], Loss: 2.3345\n",
      "Epoch [1/5], Step [4000/12500], Loss: 2.3330\n",
      "Epoch [1/5], Step [6000/12500], Loss: 2.2587\n",
      "Epoch [1/5], Step [8000/12500], Loss: 2.2464\n",
      "Epoch [1/5], Step [10000/12500], Loss: 1.9696\n",
      "Epoch [1/5], Step [12000/12500], Loss: 1.8510\n",
      "Epoch [2/5], Step [2000/12500], Loss: 1.6453\n",
      "Epoch [2/5], Step [4000/12500], Loss: 1.6203\n",
      "Epoch [2/5], Step [6000/12500], Loss: 2.3378\n",
      "Epoch [2/5], Step [8000/12500], Loss: 1.9893\n",
      "Epoch [2/5], Step [10000/12500], Loss: 2.2928\n",
      "Epoch [2/5], Step [12000/12500], Loss: 1.2138\n",
      "Epoch [3/5], Step [2000/12500], Loss: 1.1941\n",
      "Epoch [3/5], Step [4000/12500], Loss: 1.2755\n",
      "Epoch [3/5], Step [6000/12500], Loss: 1.2375\n",
      "Epoch [3/5], Step [8000/12500], Loss: 1.9386\n",
      "Epoch [3/5], Step [10000/12500], Loss: 1.2453\n",
      "Epoch [3/5], Step [12000/12500], Loss: 1.0350\n",
      "Epoch [4/5], Step [2000/12500], Loss: 1.4922\n",
      "Epoch [4/5], Step [4000/12500], Loss: 1.4428\n",
      "Epoch [4/5], Step [6000/12500], Loss: 1.8298\n",
      "Epoch [4/5], Step [8000/12500], Loss: 1.0545\n",
      "Epoch [4/5], Step [10000/12500], Loss: 1.2434\n",
      "Epoch [4/5], Step [12000/12500], Loss: 1.6190\n",
      "Epoch [5/5], Step [2000/12500], Loss: 1.0399\n",
      "Epoch [5/5], Step [4000/12500], Loss: 1.4841\n",
      "Epoch [5/5], Step [6000/12500], Loss: 1.2780\n",
      "Epoch [5/5], Step [8000/12500], Loss: 0.9433\n",
      "Epoch [5/5], Step [10000/12500], Loss: 2.3145\n",
      "Epoch [5/5], Step [12000/12500], Loss: 1.6633\n",
      "Finished Training\n",
      "Accuracy of the network: 48.81 %\n",
      "Accuracy of plane: 58.7 %\n",
      "Accuracy of car: 49.0 %\n",
      "Accuracy of bird: 38.4 %\n",
      "Accuracy of cat: 18.1 %\n",
      "Accuracy of deer: 33.2 %\n",
      "Accuracy of dog: 52.9 %\n",
      "Accuracy of frog: 62.8 %\n",
      "Accuracy of horse: 60.8 %\n",
      "Accuracy of ship: 60.0 %\n",
      "Accuracy of truck: 54.2 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "\n",
    "# Hyper-parameters \n",
    "num_epochs = 5\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "\n",
    "# dataset has PILImage images of range [0, 1]. \n",
    "# We transform them to Tensors of normalized range [-1, 1]\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# to show how class ConvNet's arguments be assign \n",
    "conv1 = nn.Conv2d(3, 6, 5)\n",
    "pool = nn.MaxPool2d(2, 2)\n",
    "conv2 = nn.Conv2d(6, 16, 5)\n",
    "print(images.shape)\n",
    "x = conv1(images)\n",
    "print(x.shape)\n",
    "x = pool(x)\n",
    "print(x.shape)\n",
    "x = conv2(x)\n",
    "print(x.shape)\n",
    "x = pool(x)\n",
    "print(x.shape)\n",
    "\n",
    "# implement convolutional neural network\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)# nn.Conv2d(input_channel_size, output_channel_size, kernel_size)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
    "        x = x.view(-1, 16 * 5 * 5)            # -> n, 400\n",
    "        x = F.relu(self.fc1(x))               # -> n, 120\n",
    "        x = F.relu(self.fc2(x))               # -> n, 84\n",
    "        x = self.fc3(x)                       # -> n, 10\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ConvNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
    "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 2000 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "PATH = './cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
